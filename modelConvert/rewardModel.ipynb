{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f114f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mohitdulani/Desktop/personal/rlhf\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preference / Reward model \n",
    "Here it learns to reward the \n",
    "\n",
    "** Dataset ** \n",
    "\n",
    "Prompt : Why is the color of sky blue ?\n",
    "Chosen : Its because of the scattering of blue wavelength by air molecules \n",
    "Rejected : Because sky likes blue color\n",
    "\n",
    "\n",
    "The reward model, is a linear layer at top, that learn to output whether this response is good or not \n",
    "\n",
    "Input1 to model : {Prompt + Chosen} ,  Output1 : 1  \n",
    "Input2 to model : {Prompt + Rejected}  , Output2: 0 \n",
    "\n",
    "\n",
    "Here the model learns to pick up a good response !\n",
    "\n",
    "This is the logic that happens in PPO loop ! So there is nothing as seperate training for a PPO model directly just use this as a Value head  \n",
    "'''\n",
    "\n",
    "# Single runnable cell — minimal, targeted fixes only (copy-paste)\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoConfig,\n",
    "    PreTrainedModel,\n",
    "    GenerationConfig\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from transformers.modeling_outputs import ModelOutput\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# ------------------ USER-CHOICE (preserved) ------------------\n",
    "BASE_MODEL = \"google/gemma-3-270m-it\"\n",
    "import os \n",
    "FILE_PATH = os.path.dirname(os.path.abspath(\"\"))\n",
    "print(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6cd881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_wrapper_with_inner(wrapper):\n",
    "    \"\"\"\n",
    "    Copy public attributes from wrapper.pretrained_model -> wrapper for compatibility.\n",
    "    Defensive: only copies attributes that don't already exist on the wrapper.\n",
    "    \"\"\"\n",
    "    inner = getattr(wrapper, \"pretrained_model\", None)\n",
    "    if inner is None:\n",
    "        raise ValueError(\"Wrapper has no attribute 'pretrained_model' to copy from.\")\n",
    "\n",
    "    # attrs to always try to copy if missing on wrapper\n",
    "    important = (\"base_model_prefix\", \"generation_config\", \"config\", \"device\", \"dtype\")\n",
    "\n",
    "    for name in dir(inner):\n",
    "        if name.startswith(\"_\"):\n",
    "            continue                 # skip private\n",
    "        if hasattr(wrapper, name):\n",
    "            continue                 # don't overwrite existing wrapper attrs\n",
    "        # skip a few problematic names that are safe to ignore\n",
    "        if name in {\"eval\", \"train\", \"to\", \"state_dict\", \"load_state_dict\"}:\n",
    "            continue\n",
    "        try:\n",
    "            setattr(wrapper, name, getattr(inner, name))\n",
    "        except Exception:\n",
    "            # ignore attributes that can't be copied (callables/properties raising, etc.)\n",
    "            pass\n",
    "\n",
    "    # ensure important ones exist and fallback to sensible defaults\n",
    "    if not hasattr(wrapper, \"base_model_prefix\"):\n",
    "        wrapper.base_model_prefix = getattr(inner, \"base_model_prefix\", \"model\")\n",
    "    if not hasattr(wrapper, \"generation_config\") and hasattr(inner, \"generation_config\"):\n",
    "        wrapper.generation_config = inner.generation_config\n",
    "    if not hasattr(wrapper, \"config\") and hasattr(inner, \"config\"):\n",
    "        wrapper.config = inner.config\n",
    "\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b256215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A <class 'transformers.models.gemma3.modeling_gemma3.Gemma3ForCausalLM'> model is loaded from 'google/gemma-3-270m-it', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the policy model config is :  GenerationConfig {\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    1,\n",
      "    106\n",
      "  ],\n",
      "  \"top_k\": 64,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A <class 'transformers.models.gemma3.modeling_gemma3.Gemma3ForCausalLM'> model is loaded from 'google/gemma-3-270m-it', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the policy model config is :  GenerationConfig {\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    1,\n",
      "    106\n",
      "  ],\n",
      "  \"top_k\": 64,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4812"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, cache_dir=FILE_PATH)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Reward model (Trained model )\n",
    "class RewardModel(PreTrainedModel):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(config)\n",
    "        # preserve your supplied model name if provided; else fall back to config\n",
    "        self.model_name = kwargs.get(\"model\", getattr(config, \"name_or_path\", BASE_MODEL))\n",
    "        # reward head => single scalar\n",
    "        self.num_labels = kwargs.get(\"num_labels\", getattr(config, \"num_labels\", 1))\n",
    "\n",
    "        # load frozen LM backbone\n",
    "        self.lm_model = AutoModelForCausalLM.from_pretrained(self.model_name, cache_dir=FILE_PATH)\n",
    "        for p in self.lm_model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # replace lm_head with scalar reward head — do NOT pass device here\n",
    "        in_features = self.lm_model.lm_head.in_features\n",
    "        self.lm_model.lm_head = nn.Linear(in_features, 1)\n",
    "        self.classifier = self.lm_model.lm_head\n",
    "        for p in self.classifier.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, return_dict: bool = True, **kwargs):\n",
    "        # pass through LM body\n",
    "        lm_out = self.lm_model.model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        last_hidden = lm_out.last_hidden_state        # [B, L, H]\n",
    "        pooled = last_hidden[:, -1, :]                # last token pooling -> [B, H]\n",
    "        rewards = self.classifier(pooled)             # [B, 1]\n",
    "        if not return_dict:\n",
    "            return (rewards,)\n",
    "        return SequenceClassifierOutput(logits=rewards)\n",
    "\n",
    "    # lightweight saving: store only head weights (as you requested earlier)\n",
    "    def save_pretrained(self, save_directory, **kwargs):\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        torch.save(self.classifier.state_dict(), os.path.join(save_directory, \"lm_head.pt\"))\n",
    "\n",
    "# class ValueOutput(ModelOutput):\n",
    "#     # convenient container: policy_logits (vocab logits), values (per-token scalar)\n",
    "#     policy_logits: torch.Tensor  # [B, L, V]\n",
    "#     values: torch.Tensor         # [B, L]\n",
    "\n",
    "# class ValueModel(PreTrainedModelWrapper):\n",
    "#     \"\"\"\n",
    "#     Keeps lm_head (policy logits) and adds a separate value_head (per-token values).\n",
    "#     \"\"\"\n",
    "#     base_model_prefix = \"lm_model\"  # required for HF compatibility\n",
    "\n",
    "#     def __init__(self, config, model: str =BASE_MODEL, freeze_backbone: bool = True, **kwargs):\n",
    "#         num_labels = kwargs.get('num_labels', 1)\n",
    "#         lm_model = AutoModelForCausalLM.from_pretrained(model, cache_dir=FILE_PATH)\n",
    "#         # NOTE: we pass the model instance to the wrapper\n",
    "#         super().__init__(lm_model)\n",
    "\n",
    "\n",
    "#         self.lm_model = lm_model\n",
    "#         self.generation_config = self.lm_model.config\n",
    "\n",
    "#         # If you intend to train the policy (PPO), DON'T freeze the policy model here.\n",
    "#         # But the ValueModel is typically a separate object from the policy — freeze its backbone so only\n",
    "#         # the value head trains. If you are using the same model object for policy+value, manage freezing carefully.\n",
    "#         if freeze_backbone:\n",
    "#             for p in self.lm_model.parameters():\n",
    "#                 p.requires_grad = False\n",
    "\n",
    "#         hidden_size = getattr(self.lm_model.config, \"hidden_size\", None) or getattr(self.lm_model.config, \"d_model\", None)\n",
    "#         if hidden_size is None:\n",
    "#             raise ValueError(\"Couldn't infer hidden size from model config\")\n",
    "\n",
    "#         # value head produces one scalar per token -> [B, L, 1] -> squeeze -> [B, L]\n",
    "\n",
    "#         # self.value_head = nn.Sequential(\n",
    "#         #     nn.Softmax(dim = -1),\n",
    "#         #     self.lm_model.model.embed_tokens(), \n",
    "#         #     nn.Linear(hidden_size, num_labels),\n",
    "#         # )\n",
    "\n",
    "#         self.value_head = nn.Linear(hidden_size, 1)\n",
    "#         for p in self.value_head.parameters():\n",
    "#             p.requires_grad = True\n",
    "\n",
    "#     def __getattr__(self, name):\n",
    "#         # fallback: delegate to lm_model\n",
    "#         try:\n",
    "#             return super().__getattr__(name)\n",
    "#         except AttributeError:\n",
    "#             return getattr(self.lm_model, name)\n",
    "\n",
    "#     def _get_transformer_body(self):\n",
    "#         # common names; Gemma3 appears to have .model as the body\n",
    "#         for attr in (\"base_model\", \"model\", \"transformer\", \"gpt_neox\", \"decoder\"):\n",
    "#             candidate = getattr(self.lm_model, attr, None)\n",
    "#             if candidate is not None:\n",
    "#                 return candidate\n",
    "#         return self.lm_model\n",
    "\n",
    "#     def forward(self, input_ids=None, attention_mask=None, return_dict: bool = True, **kwargs):\n",
    "#         \"\"\"\n",
    "#         Returns:\n",
    "#           - policy_logits: LM vocab logits [B, L, V] (from lm_head applied to last_hidden_state)\n",
    "#           - values: per-token scalar values [B, L]\n",
    "#         Use 'policy_logits' to compute token probabilities, sample actions, compute log-probs etc.\n",
    "#         \"\"\"\n",
    "#         transformer = self._get_transformer_body()\n",
    "\n",
    "#         # run transformer body only (avoid double heads); returns last_hidden_state [B, L, H]\n",
    "#         outputs = transformer(input_ids=input_ids, attention_mask=attention_mask, return_dict=True, **kwargs)\n",
    "#         last_hidden = getattr(outputs, \"last_hidden_state\", None)\n",
    "#         if last_hidden is None:\n",
    "#             last_hidden = outputs[0]\n",
    "\n",
    "#         # 1) policy logits via the original lm_head (do NOT replace lm_head)\n",
    "#         #    We call lm_head on last_hidden directly (this is equivalent to lm_model.forward's logits)\n",
    "#         policy_logits = self.lm_model.lm_head(last_hidden)  # [B, L, V]\n",
    "\n",
    "#         # 2) value head -> per-token scalar\n",
    "#         values = self.value_head(last_hidden).squeeze(-1)  # [B, L]\n",
    "\n",
    "#         if not return_dict:\n",
    "#             return (policy_logits, values)\n",
    "\n",
    "#         return ValueOutput(policy_logits=policy_logits, values=values)\n",
    "\n",
    "#     # helper: compute chosen-token log-probs given token ids (useful for PPO loss / KL)\n",
    "#     @staticmethod\n",
    "#     def selected_token_logprobs(policy_logits: torch.Tensor, chosen_tokens: torch.Tensor):\n",
    "#         \"\"\"\n",
    "#         policy_logits: [B, L, V]\n",
    "#         chosen_tokens:  [B, L]  (token ids for each position; -100 or padding for positions to ignore)\n",
    "#         returns: selected_logprobs [B, L] (log-prob of the chosen token at each position)\n",
    "#         \"\"\"\n",
    "#         log_probs = F.log_softmax(policy_logits, dim=-1)               # [B, L, V]\n",
    "#         chosen = chosen_tokens.unsqueeze(-1)                           # [B, L, 1]\n",
    "#         selected_logprobs = torch.gather(log_probs, dim=-1, index=chosen).squeeze(-1)  # [B, L]\n",
    "#         return selected_logprobs\n",
    "\n",
    "config = AutoConfig.from_pretrained(BASE_MODEL, num_labels=1, cache_dir=FILE_PATH)\n",
    "\n",
    "\n",
    "# Policy model (with value head)\n",
    "policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(BASE_MODEL)\n",
    "\n",
    "\n",
    "# Attach generation config to the inner LM\n",
    "policy_model.generation_config = GenerationConfig.from_pretrained(BASE_MODEL)\n",
    "print(\"the policy model config is : \" , policy_model.pretrained_model.generation_config )\n",
    "# policy_model.pretrained_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "# policy_model.pretrained_model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# optional safety: if some code later looks for base_model_prefix on the wrapper, patch it\n",
    "if not hasattr(policy_model, \"base_model_prefix\"):\n",
    "    # prefer inner model's prefix if available, otherwise default to \"model\"\n",
    "    policy_model.base_model_prefix = getattr(policy_model.pretrained_model, \"base_model_prefix\", \"model\")\n",
    "\n",
    "\n",
    "\n",
    "# Value model (with value head) – optional\n",
    "value_model = AutoModelForCausalLMWithValueHead.from_pretrained(BASE_MODEL)\n",
    "value_model.generation_config = GenerationConfig.from_pretrained(BASE_MODEL)\n",
    "print(\"the policy model config is : \" , value_model.pretrained_model.generation_config )\n",
    "# value_model.pretrained_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "# value_model.pretrained_model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "if not hasattr(value_model, \"base_model_prefix\"):\n",
    "    # prefer inner model's prefix if available, otherwise default to \"model\"\n",
    "    value_model.base_model_prefix = getattr(value_model.pretrained_model, \"base_model_prefix\", \"model\")\n",
    "\n",
    "policy_model = patch_wrapper_with_inner(policy_model)\n",
    "value_model = patch_wrapper_with_inner(value_model)\n",
    "\n",
    "reward_model = RewardModel(config, model=BASE_MODEL, num_labels=1)\n",
    "\n",
    "# pick device\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "policy_model.to(device)\n",
    "value_model.to(device)\n",
    "reward_model.to(device)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c093a7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the policy model config is :  GenerationConfig {\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"top_k\": 64,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"the policy model config is : \" , policy_model.generation_config )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "025db6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Dahoas/rm-static\", split=\"train[:20]\", cache_dir=FILE_PATH)\n",
    "\n",
    "\n",
    "# Convert into a format PPOTrainer expects\n",
    "def preprocess(examples):\n",
    "    prompt = examples['prompt']\n",
    "    out = tokenizer.__call__(prompt)\n",
    "    return out\n",
    "\n",
    "dataset = dataset.map(preprocess, remove_columns=dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90a409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer, PPOConfig\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# 2) PPO config\n",
    "# ppo_config = PPOConfig(\n",
    "#     learning_rate=1.41e-5,\n",
    "#     batch_size=16,\n",
    "#     mini_batch_size=4,\n",
    "#     logging_strategy = 'steps' , \n",
    "#     logging_steps = 10, \n",
    "#     report_to = 'wandb',\n",
    "#     gradient_accumulation_steps=1\n",
    "# )\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    learning_rate=1e-6,               # lower LR\n",
    "    batch_size=16, mini_batch_size=4,\n",
    "    report_to=\"wandb\",\n",
    "    fp16=False, bf16=False,        # turn off AMP\n",
    "    gradient_accumulation_steps=1,\n",
    "    cliprange=0.1, \n",
    "    cliprange_value=0.2,\n",
    "\n",
    "    # pad_token_id=tokenizer.pad_token_id,\n",
    "    \n",
    "    stop_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "\n",
    "# https://huggingface.co/docs/trl/v0.8.1/en/ppo_trainer#ppo-trainer\n",
    "# 4) PPO Trainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model = policy_model,\n",
    "    reward_model=reward_model,\n",
    "    value_model=value_model,\n",
    "    ref_model = None, \n",
    "    args=ppo_config,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "'''\n",
    "update_model.to(device)\n",
    "value_model.to(device)\n",
    "conv_model.to(device)\n",
    "'''\n",
    "\n",
    "\n",
    "ppo_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a95ee3",
   "metadata": {},
   "source": [
    "## end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727ab021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:asyncio:socket.send() raised exception.\n",
      "WARNING:asyncio:socket.send() raised exception.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 108, 36623, 236787, 3199, 611, 9779, 506, 6555, 531, 3980, 118074, 532, 1406, 209291, 699, 496, 12836, 4322, 108, 74314, 236787, 8438, 236764, 8454, 236761, 2282, 3980, 822, 4322, 236764, 611, 1171, 1202, 531, 1161, 496, 187110, 20182, 653, 3538, 236764, 31225, 20182, 531, 29846, 49616, 1679, 506, 3761, 529, 506, 4322, 236761, 12067, 236764, 611, 236858, 859, 1461, 531, 14304, 496, 3538, 236764, 60250, 236772, 6796, 236764, 187110, 12325, 20182, 532, 29846, 8706, 625, 1063, 532, 12034, 3418, 506, 4322, 531, 6349, 118074, 532, 1406, 209291, 236761, 108, 36623, 236787, 3199, 564, 15414, 152814, 10812, 8990, 506, 20182, 532, 3980, 625, 600, 1595, 236881, 108, 74314, 236787], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:asyncio:socket.send() raised exception.\n",
      "WARNING:asyncio:socket.send() raised exception.\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "# from transformers import AutoTokenizer, pipeline\n",
    "# from trl import PPOTrainer, PPOConfig\n",
    "# from trl.core import LengthSampler\n",
    "\n",
    "\n",
    "# # 1) load dataset for PPO training\n",
    "# dataset = load_dataset(\"Dahoas/rm-static\", split=\"train[:2000]\", cache_dir=FILE_PATH)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# # Convert into a format PPOTrainer expects\n",
    "# def preprocess(examples):\n",
    "#     prompt = examples['prompt']\n",
    "#     out = tokenizer.__call__(prompt)\n",
    "#     return out\n",
    "\n",
    "# dataset = dataset.map(preprocess, remove_columns=dataset.column_names)\n",
    "\n",
    "# # Now dataset is a Dataset with {\"query\": \"...\"}\n",
    "# print(dataset[0])  # {'query': 'some prompt text'}\n",
    "\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "734f20e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:asyncio:socket.send() raised exception.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Column(['\\n\\nHuman: Can you describe the steps to clean fingerprints and smudges from a laptop screen\\n\\nAssistant: Yes, certainly. To clean your screen, you first need to use a microfiber cloth or soft, damp cloth to gently wipe down the surface of the screen. Next, you’ll want to grab a soft, lint-free, microfiber cleaning cloth and gently rub it back and forth across the screen to remove fingerprints and smudges.\\n\\nHuman: Can I spray isopropyl alcohol onto the cloth and clean it that way?\\n\\nAssistant:', '\\n\\nHuman: What are some foods that are good for diabetics?\\n\\nAssistant: To be honest, some of these are better than others, and they’re a little more like opinions than facts. For example, many of the diets say to limit vegetables with high sugar content, and there’s some debate on the subject, as far as how much of these vegetables are actually bad for diabetics.\\n\\nHuman: Okay, any other advice?\\n\\nAssistant:', \"\\n\\nHuman: What animal would be the dominate life form on Earth if humans weren't here?\\n\\nAssistant: Most life on Earth would be taken over by bacteria and insects.\\n\\nHuman: What about birds? Could they ever come to dominate the Earth?\\n\\nAssistant:\", '\\n\\nHuman: How often are the Olympics?\\n\\nAssistant:', '\\n\\nHuman: Can I use car wax on my linoleum floor to make it shine?\\n\\nAssistant:'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:asyncio:socket.send() raised exception.\n",
      "WARNING:asyncio:socket.send() raised exception.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:asyncio:socket.send() raised exception.\n"
     ]
    }
   ],
   "source": [
    "dataset['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59a065aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nHuman: Can you describe the steps to clean fingerprints and smudges from a laptop screen\\n\\nAssistant: Yes, certainly. To clean your screen, you first need to use a microfiber cloth or soft, damp cloth to gently wipe down the surface of the screen. Next, you’ll want to grab a soft, lint-free, microfiber cleaning cloth and gently rub it back and forth across the screen to remove fingerprints and smudges.\\n\\nHuman: Can I spray isopropyl alcohol onto the cloth and clean it that way?\\n\\nAssistant:'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084662ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tokenizer.__call__(dataset)\n",
    "\n",
    "out = tokenizer.__call__(dataset[0]['query'])\n",
    "print(out.input_ids , type(out), )\n",
    "\n",
    "pt = tokenizer.prepare_for_model(out.input_ids)\n",
    "print(pt)\n",
    "\n",
    "# tokenizer.prepare_for_model\n",
    "\n",
    "\n",
    "ds = pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2f593f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:asyncio:socket.send() raised exception.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Gemma3ForCausalLM(\n  (model): Gemma3TextModel(\n    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n    (layers): ModuleList(\n      (0-17): 18 x Gemma3DecoderLayer(\n        (self_attn): Gemma3Attention(\n          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n        )\n        (mlp): Gemma3MLP(\n          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n          (act_fn): PytorchGELUTanh()\n        )\n        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n      )\n    )\n    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n    (rotary_emb): Gemma3RotaryEmbedding()\n    (rotary_emb_local): Gemma3RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n) argument after ** must be a mapping, not Dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m enc = dataset\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     out = update_model(**enc)          \u001b[38;5;66;03m# logits check\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m torch.isfinite(out.logits).all()\n\u001b[32m     15\u001b[39m     _ = update_model.generate(**enc, **gen_kwargs)\n",
      "\u001b[31mTypeError\u001b[39m: Gemma3ForCausalLM(\n  (model): Gemma3TextModel(\n    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n    (layers): ModuleList(\n      (0-17): 18 x Gemma3DecoderLayer(\n        (self_attn): Gemma3Attention(\n          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n        )\n        (mlp): Gemma3MLP(\n          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n          (act_fn): PytorchGELUTanh()\n        )\n        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n      )\n    )\n    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n    (rotary_emb): Gemma3RotaryEmbedding()\n    (rotary_emb_local): Gemma3RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n) argument after ** must be a mapping, not Dataset"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:asyncio:socket.send() raised exception.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:asyncio:socket.send() raised exception.\n",
      "WARNING:asyncio:socket.send() raised exception.\n"
     ]
    }
   ],
   "source": [
    "gen_kwargs = dict(\n",
    "    max_new_tokens=64,            # keep small\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=1.0,              # avoid 0 or <0\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "# enc = tokenizer(dataset[0][\"query\"], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "enc = dataset\n",
    "with torch.no_grad():\n",
    "    out = update_model(**enc)          # logits check\n",
    "    assert torch.isfinite(out.logits).all()\n",
    "    _ = update_model.generate(**enc, **gen_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f30c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:asyncio:socket.send() raised exception.\n",
      "WARNING:asyncio:socket.send() raised exception.\n",
      "WARNING:asyncio:socket.send() raised exception.\n",
      "WARNING:asyncio:socket.send() raised exception.\n",
      "WARNING:asyncio:socket.send() raised exception.\n",
      "WARNING:asyncio:socket.send() raised exception.\n",
      "WARNING:asyncio:socket.send() raised exception.\n",
      "WARNING:asyncio:socket.send() raised exception.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===training policy===\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     33\u001b[39m ppo_trainer = PPOTrainer(\n\u001b[32m     34\u001b[39m     model = update_model,\n\u001b[32m     35\u001b[39m     reward_model=conv_model,\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     train_dataset=dataset,\n\u001b[32m     41\u001b[39m )\n\u001b[32m     43\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[33;03mupdate_model.to(device)\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[33;03mvalue_model.to(device)\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[33;03mconv_model.to(device)\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43mppo_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/personal/rlhf/.venv/lib/python3.13/site-packages/trl/trainer/ppo_trainer.py:428\u001b[39m, in \u001b[36mPPOTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    424\u001b[39m values = []\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m unwrap_model_for_generation(\n\u001b[32m    426\u001b[39m     \u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.accelerator, gather_deepspeed3_params=\u001b[38;5;28mself\u001b[39m.args.ds3_gather_for_generation\n\u001b[32m    427\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m unwrapped_model:\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m     query_responses, logitss = \u001b[43mbatch_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m        \u001b[49m\u001b[43munwrapped_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m        \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlocal_rollout_forward_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, queries.shape[\u001b[32m0\u001b[39m], args.local_rollout_forward_batch_size):\n\u001b[32m    437\u001b[39m     query = queries[i : i + args.local_rollout_forward_batch_size]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/personal/rlhf/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/personal/rlhf/.venv/lib/python3.13/site-packages/trl/trainer/utils.py:1122\u001b[39m, in \u001b[36mbatch_generation\u001b[39m\u001b[34m(model, queries, local_rollout_forward_batch_size, pad_token_id, generation_config)\u001b[39m\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, batch_size, local_rollout_forward_batch_size):\n\u001b[32m   1121\u001b[39m     query = queries[i : i + local_rollout_forward_batch_size]\n\u001b[32m-> \u001b[39m\u001b[32m1122\u001b[39m     query_response, logits = \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1124\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1128\u001b[39m     query_responses.append(query_response)\n\u001b[32m   1129\u001b[39m     logitss.append(logits)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/personal/rlhf/.venv/lib/python3.13/site-packages/trl/trainer/utils.py:1096\u001b[39m, in \u001b[36mgenerate\u001b[39m\u001b[34m(lm_backbone, queries, pad_token_id, generation_config)\u001b[39m\n\u001b[32m   1094\u001b[39m attention_mask = queries != pad_token_id\n\u001b[32m   1095\u001b[39m input_ids = torch.masked_fill(queries, ~attention_mask, \u001b[32m0\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1096\u001b[39m output = \u001b[43mlm_backbone\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1098\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1099\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# position_ids=attention_mask.cumsum(1) - attention_mask.long(), # not needed: already adjusted in generations\u001b[39;49;00m\n\u001b[32m   1100\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# https://github.com/huggingface/transformers/blob/ac33aeeeee2a7a89b89c93c2962e6feb90daef0a/src/transformers/models/gpt2/modeling_gpt2.py#L1227-L1250\u001b[39;49;00m\n\u001b[32m   1101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m logits = torch.stack(output.scores, \u001b[32m1\u001b[39m)\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat((queries, output.sequences[:, context_length:]), dim=\u001b[32m1\u001b[39m), logits\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/personal/rlhf/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/personal/rlhf/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2539\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2528\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m GenerationMixin.generate(\n\u001b[32m   2529\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2530\u001b[39m         inputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2534\u001b[39m         **kwargs,\n\u001b[32m   2535\u001b[39m     )\n\u001b[32m   2537\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n\u001b[32m   2538\u001b[39m     \u001b[38;5;66;03m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2539\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2540\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2541\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2542\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2543\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2544\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2546\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2547\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2549\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2550\u001b[39m     \u001b[38;5;66;03m# 11. run beam sample\u001b[39;00m\n\u001b[32m   2551\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._beam_search(\n\u001b[32m   2552\u001b[39m         input_ids,\n\u001b[32m   2553\u001b[39m         logits_processor=prepared_logits_processor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2557\u001b[39m         **model_kwargs,\n\u001b[32m   2558\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/personal/rlhf/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2912\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2910\u001b[39m     probs = nn.functional.softmax(next_token_scores, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m   2911\u001b[39m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2912\u001b[39m     next_tokens = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m.squeeze(\u001b[32m1\u001b[39m)\n\u001b[32m   2913\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2914\u001b[39m     next_tokens = torch.argmax(next_token_scores, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:asyncio:socket.send() raised exception.\n",
      "WARNING:asyncio:socket.send() raised exception.\n"
     ]
    }
   ],
   "source": [
    "from trl import PPOTrainer, PPOConfig\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# 2) PPO config\n",
    "# ppo_config = PPOConfig(\n",
    "#     learning_rate=1.41e-5,\n",
    "#     batch_size=16,\n",
    "#     mini_batch_size=4,\n",
    "#     logging_strategy = 'steps' , \n",
    "#     logging_steps = 10, \n",
    "#     report_to = 'wandb',\n",
    "#     gradient_accumulation_steps=1\n",
    "# )\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    learning_rate=5e-6,               # lower LR\n",
    "    batch_size=16, mini_batch_size=4,\n",
    "    report_to=\"wandb\",\n",
    "    fp16=False, bf16=False,        # turn off AMP\n",
    "    gradient_accumulation_steps=1,\n",
    "    cliprange=0.1, \n",
    "    cliprange_value=0.2,\n",
    ")\n",
    "\n",
    "\n",
    "# https://huggingface.co/docs/trl/v0.8.1/en/ppo_trainer#ppo-trainer\n",
    "# 4) PPO Trainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model = update_model,\n",
    "    reward_model=conv_model,\n",
    "    value_model=value_model,\n",
    "    ref_model = None, \n",
    "    args=ppo_config,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "'''\n",
    "update_model.to(device)\n",
    "value_model.to(device)\n",
    "conv_model.to(device)\n",
    "'''\n",
    "\n",
    "\n",
    "ppo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeff57ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4969b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['query']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     49\u001b[39m output_max_len = \u001b[32m64\u001b[39m\n\u001b[32m     50\u001b[39m output_length_sampler = LengthSampler(output_min_len, output_max_len)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mppo_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_tensors\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mdesktop/rlhf/.venv/lib/python3.11/site-packages/accelerate/data_loader.py:567\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m     current_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    569\u001b[39m     \u001b[38;5;28mself\u001b[39m.end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mdesktop/rlhf/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mdesktop/rlhf/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mdesktop/rlhf/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mdesktop/rlhf/.venv/lib/python3.11/site-packages/transformers/data/data_collator.py:272\u001b[39m, in \u001b[36mDataCollatorWithPadding.__call__\u001b[39m\u001b[34m(self, features)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m     batch = \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m    281\u001b[39m         batch[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m] = batch[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mdesktop/rlhf/.venv/lib/python3.11/site-packages/transformers/data/data_collator.py:67\u001b[39m, in \u001b[36mpad_without_fast_tokenizer_warning\u001b[39m\u001b[34m(tokenizer, *pad_args, **pad_kwargs)\u001b[39m\n\u001b[32m     64\u001b[39m tokenizer.deprecation_warnings[\u001b[33m\"\u001b[39m\u001b[33mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     padded = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[32m     70\u001b[39m     tokenizer.deprecation_warnings[\u001b[33m\"\u001b[39m\u001b[33mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[33m\"\u001b[39m] = warning_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mdesktop/rlhf/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3347\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.pad\u001b[39m\u001b[34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[39m\n\u001b[32m   3345\u001b[39m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has been passed for padding\u001b[39;00m\n\u001b[32m   3346\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_input_names[\u001b[32m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[32m-> \u001b[39m\u001b[32m3347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3348\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3349\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model_input_names[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(encoded_inputs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   3350\u001b[39m     )\n\u001b[32m   3352\u001b[39m required_input = encoded_inputs[\u001b[38;5;28mself\u001b[39m.model_input_names[\u001b[32m0\u001b[39m]]\n\u001b[32m   3354\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m required_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(required_input, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(required_input) == \u001b[32m0\u001b[39m):\n",
      "\u001b[31mValueError\u001b[39m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['query']"
     ]
    }
   ],
   "source": [
    "from trl import PPOTrainer, PPOConfig\n",
    "import inspect\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Correctly use importlib to import PPOTrainer\n",
    "importlib.import_module('trl.trainer.ppo_trainer')\n",
    "\n",
    "# sys.modules.pop('PPOTrainer', None)\n",
    "\n",
    "\n",
    "# Print the signature of PPOTrainer in a nicely formatted way\n",
    "ppo_trainer_signature = inspect.signature(PPOTrainer)\n",
    "\n",
    "\n",
    "# 2) PPO config\n",
    "ppo_config = PPOConfig(\n",
    "    learning_rate=1.41e-5,\n",
    "    batch_size=16,\n",
    "    mini_batch_size=4,\n",
    "    gradient_accumulation_steps=1\n",
    ")\n",
    "\n",
    "# 3) reward pipeline (uses your trained conv_model as RM)\n",
    "def compute_rewards(samples, responses):\n",
    "    # samples: original prompts\n",
    "    # responses: generated model responses\n",
    "    texts = [s + r for s, r in zip(samples, responses)]\n",
    "    toks = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = conv_model(**toks).logits.squeeze(-1)\n",
    "\n",
    "    return scores  # shape: (batch,)\n",
    "\n",
    "# 4) PPO Trainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model = value_model,\n",
    "    ref_model = None, \n",
    "    args=ppo_config,\n",
    "    value_model=value_model,\n",
    "    reward_model = conv_model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# 5) main training loop\n",
    "output_min_len = 16\n",
    "output_max_len = 64\n",
    "output_length_sampler = LengthSampler(output_min_len, output_max_len)\n",
    "\n",
    "for epoch, batch in enumerate(ppo_trainer.dataloader):\n",
    "    queries = batch[\"query\"] if isinstance(batch[\"query\"], list) else [batch[\"query\"]]\n",
    "    query_tensors = tokenizer(\n",
    "        queries,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    ).input_ids.to(device)\n",
    "    # query_tensors = tokenizer(batch[\"query\"], return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "    # sample responses from the policy (actor)\n",
    "    response_tensors = ppo_trainer.generate(query_tensors, max_new_tokens=output_length_sampler())\n",
    "    responses = tokenizer.batch_decode\n",
    "    (response_tensors, skip_special_tokens=True)\n",
    "\n",
    "    # get reward scores from reward model\n",
    "    rewards = compute_rewards(batch[\"query\"], responses)\n",
    "\n",
    "    # PPO step (policy + value update)\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "print(\"✅ PPO training loop finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10dc17cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: {'query': '\\n\\nHuman: Can you describe the steps to clean fingerprints and smudges from a laptop screen\\n\\nAssistant: Yes, certainly. To clean your screen, you first need to use a microfiber cloth or soft, damp cloth to gently wipe down the surface of the screen. Next, you’ll want to grab a soft, lint-free, microfiber cleaning cloth and gently rub it back and forth across the screen to remove fingerprints and smudges.\\n\\nHuman: Can I spray isopropyl alcohol onto the cloth and clean it that way?\\n\\nAssistant:'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from trl import PPOTrainer, PPOConfig\n",
    "from transformers import AutoTokenizer\n",
    "from trl.core import LengthSampler\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\")  # or \"cuda\"\n",
    "\n",
    "# === 1) dataset: keep it as a HF Dataset and map to a single 'query' field ===\n",
    "ds = load_dataset(\"Dahoas/rm-static\", split=\"train[:2000]\", cache_dir=FILE_PATH)\n",
    "\n",
    "def keep_query(examples):\n",
    "    return {\"query\": examples[\"prompt\"]}\n",
    "\n",
    "ds = ds.map(keep_query, remove_columns=ds.column_names)  # now every row: {\"query\": \"...\"}\n",
    "# sanity\n",
    "print(\"example:\", ds[0])\n",
    "\n",
    "# === 2) tokenizer ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# === 3) collator: ALWAYS return a dict with 'query' -> list[str] ===\n",
    "def collator(batch):\n",
    "    # batch: list of dicts like [{'query': '...'}, ...]\n",
    "    queries = [b[\"query\"] for b in batch]\n",
    "    return {\"query\": queries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5542b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: {'query': '\\n\\nHuman: Can you describe the steps to clean fingerprints and smudges from a laptop screen\\n\\nAssistant: Yes, certainly. To clean your screen, you first need to use a microfiber cloth or soft, damp cloth to gently wipe down the surface of the screen. Next, you’ll want to grab a soft, lint-free, microfiber cleaning cloth and gently rub it back and forth across the screen to remove fingerprints and smudges.\\n\\nHuman: Can I spray isopropyl alcohol onto the cloth and clean it that way?\\n\\nAssistant:'}\n",
      "starting the training loop here \n",
      "===training policy===\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     71\u001b[39m     ppo_trainer.reward_fn = compute_rewards\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# Now call push-button train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[43mppo_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# this should run the full PPO pipeline: generate → reward → step → update\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mdesktop/rlhf/.venv/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:416\u001b[39m, in \u001b[36mPPOTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    414\u001b[39m data = \u001b[38;5;28mnext\u001b[39m(iter_dataloader)\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     queries = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m(device)\n\u001b[32m    417\u001b[39m     context_length = queries.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    418\u001b[39m     responses = []\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# === 4) instantiate (make sure ref_model exists if your constructor requires it) ===\n",
    "# if your PPOTrainer signature expects args first (like your custom class), adapt accordingly.\n",
    "# Typical TRL usage:\n",
    "ref_model = copy.deepcopy(value_model)\n",
    "ref_model.to(device)\n",
    "ref_model.eval()\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    args=ppo_config,                # or config=ppo_config depending on your class\n",
    "    processing_class=tokenizer,     # some custom class uses this name\n",
    "    model=value_model,\n",
    "    value_model=value_model,\n",
    "    ref_model=ref_model,            # provide a frozen reference model if required\n",
    "    reward_model=conv_model,\n",
    "    train_dataset=ds,               # pass the HF Dataset, NOT ds['query']\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "# === 5) reward function: conv_model returns scalar per sequence ===\n",
    "def compute_rewards(queries, responses):\n",
    "    # queries: list[str], responses: list[str]\n",
    "    texts = [q + r for q, r in zip(queries, responses)]\n",
    "    toks = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        scores = conv_model(input_ids=toks[\"input_ids\"], attention_mask=toks[\"attention_mask\"]).logits\n",
    "        # squeeze if shape [B, L, 1] or [B,1]\n",
    "        scores = scores.squeeze()\n",
    "        if scores.dim() == 2:  # shape [B, L] -> reduce if needed (use final token or sum)\n",
    "            scores = scores[:, -1]  \n",
    "    return scores.detach().cpu()\n",
    "\n",
    "# === 6) training loop: be defensive about batch types ===\n",
    "\n",
    "print(\"starting the training loop here \")\n",
    "output_min_len, output_max_len = 16, 64\n",
    "output_length_sampler = LengthSampler(output_min_len, output_max_len)\n",
    "\n",
    "if not hasattr(ppo_trainer, \"reward_fn\"):\n",
    "    ppo_trainer.reward_fn = compute_rewards\n",
    "else:\n",
    "    ppo_trainer.reward_fn = compute_rewards\n",
    "\n",
    "# Now call push-button train\n",
    "ppo_trainer.train()   # this should run the full PPO pipeline: generate → reward → step → update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55c10fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacity of 7.63 GiB of which 516.44 MiB is free. Including non-PyTorch memory, this process has 7.10 GiB memory in use. Of the allocated memory 6.99 GiB is allocated by PyTorch, and 8.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m conv_model.to(device)\n\u001b[32m     12\u001b[39m conv_model.eval()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m ref_model = \u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m ref_model.to(device)\n\u001b[32m     15\u001b[39m ref_model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:172\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    170\u001b[39m                 y = x\n\u001b[32m    171\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m                 y = \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:271\u001b[39m, in \u001b[36m_reconstruct\u001b[39m\u001b[34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m         state = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[33m'\u001b[39m\u001b[33m__setstate__\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    273\u001b[39m         y.__setstate__(state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:146\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    144\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:231\u001b[39m, in \u001b[36m_deepcopy_dict\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    229\u001b[39m memo[\u001b[38;5;28mid\u001b[39m(x)] = y\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     y[deepcopy(key, memo)] = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:146\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    144\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:231\u001b[39m, in \u001b[36m_deepcopy_dict\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    229\u001b[39m memo[\u001b[38;5;28mid\u001b[39m(x)] = y\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     y[deepcopy(key, memo)] = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:172\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    170\u001b[39m                 y = x\n\u001b[32m    171\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m                 y = \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:271\u001b[39m, in \u001b[36m_reconstruct\u001b[39m\u001b[34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m         state = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[33m'\u001b[39m\u001b[33m__setstate__\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    273\u001b[39m         y.__setstate__(state)\n",
      "    \u001b[31m[... skipping similar frames: _deepcopy_dict at line 231 (4 times), deepcopy at line 146 (4 times), _reconstruct at line 271 (1 times), deepcopy at line 172 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:172\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    170\u001b[39m                 y = x\n\u001b[32m    171\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m                 y = \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:271\u001b[39m, in \u001b[36m_reconstruct\u001b[39m\u001b[34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m         state = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[33m'\u001b[39m\u001b[33m__setstate__\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    273\u001b[39m         y.__setstate__(state)\n",
      "    \u001b[31m[... skipping similar frames: _deepcopy_dict at line 231 (1 times), deepcopy at line 146 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:146\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    144\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:231\u001b[39m, in \u001b[36m_deepcopy_dict\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    229\u001b[39m memo[\u001b[38;5;28mid\u001b[39m(x)] = y\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     y[deepcopy(key, memo)] = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:153\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    151\u001b[39m copier = \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33m__deepcopy__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    155\u001b[39m     reductor = dispatch_table.get(\u001b[38;5;28mcls\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mdesktop/rlhf/.venv/lib/python3.11/site-packages/torch/nn/parameter.py:68\u001b[39m, in \u001b[36mParameter.__deepcopy__\u001b[39m\u001b[34m(self, memo)\u001b[39m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)]\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     67\u001b[39m     result = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.requires_grad\n\u001b[32m     69\u001b[39m     )\n\u001b[32m     70\u001b[39m     memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)] = result\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacity of 7.63 GiB of which 516.44 MiB is free. Including non-PyTorch memory, this process has 7.10 GiB memory in use. Of the allocated memory 6.99 GiB is allocated by PyTorch, and 8.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import copy, inspect, torch\n",
    "from trl import PPOTrainer\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "# device, dataset, tokenizer, value_model, conv_model, ppo_config must already exist\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() and torch.device(\"mps\") else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# quick safety: move models to device\n",
    "value_model.to(device)\n",
    "conv_model.to(device)\n",
    "conv_model.eval()\n",
    "ref_model = copy.deepcopy(value_model)\n",
    "ref_model.to(device)\n",
    "ref_model.eval()\n",
    "\n",
    "# make sure ds is a HF Dataset with 'query' column\n",
    "print(\"example dataset row:\", ds[0])\n",
    "\n",
    "# collator that returns list[str] under 'query'\n",
    "def collator(batch):\n",
    "    return {\"query\": [b[\"query\"] for b in batch]}\n",
    "\n",
    "# inspect signature and build kwargs\n",
    "sig = inspect.signature(PPOTrainer.__init__)\n",
    "params = list(sig.parameters.keys())\n",
    "print(\"PPOTrainer.__init__ params:\", params)\n",
    "\n",
    "kw = {}\n",
    "# config / args\n",
    "if \"config\" in params:\n",
    "    kw[\"config\"] = ppo_config\n",
    "elif \"args\" in params:\n",
    "    kw[\"args\"] = ppo_config\n",
    "else:\n",
    "    raise RuntimeError(\"Can't find 'config' or 'args' in PPOTrainer signature; inspect and adapt.\")\n",
    "\n",
    "# tokenizer / processing_class\n",
    "if \"tokenizer\" in params:\n",
    "    kw[\"tokenizer\"] = tokenizer\n",
    "elif \"processing_class\" in params:\n",
    "    kw[\"processing_class\"] = tokenizer\n",
    "\n",
    "# required models\n",
    "if \"model\" in params:\n",
    "    kw[\"model\"] = value_model  # often the actor/policy (if your trainer expects a separate value_model, we set that below)\n",
    "if \"ref_model\" in params:\n",
    "    kw[\"ref_model\"] = ref_model\n",
    "if \"reward_model\" in params:\n",
    "    kw[\"reward_model\"] = conv_model\n",
    "# IMPORTANT: some local/trl forks expect a separate 'value_model' parameter\n",
    "if \"value_model\" in params:\n",
    "    kw[\"value_model\"] = value_model\n",
    "\n",
    "# dataset param name\n",
    "if \"dataset\" in params:\n",
    "    kw[\"dataset\"] = ds\n",
    "elif \"train_dataset\" in params:\n",
    "    kw[\"train_dataset\"] = ds\n",
    "\n",
    "# data collator\n",
    "if \"data_collator\" in params:\n",
    "    kw[\"data_collator\"] = collator\n",
    "\n",
    "print(\"Instantiating PPOTrainer with kwargs:\", list(kw.keys()))\n",
    "ppo_trainer = PPOTrainer(**kw)\n",
    "\n",
    "# attach a reward function (safe)\n",
    "def compute_rewards(queries, responses):\n",
    "    texts = [q + r for q, r in zip(queries, responses)]\n",
    "    toks = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = conv_model(input_ids=toks[\"input_ids\"], attention_mask=toks[\"attention_mask\"])\n",
    "    scores = getattr(out, \"logits\", out)\n",
    "    scores = torch.as_tensor(scores).squeeze()\n",
    "    if scores.dim() == 2:\n",
    "        if scores.shape[1] == 1:\n",
    "            scores = scores[:, 0]\n",
    "        else:\n",
    "            scores = scores[:, -1]\n",
    "    return scores.cpu()\n",
    "\n",
    "ppo_trainer.reward_fn = compute_rewards\n",
    "\n",
    "print(\"Calling ppo_trainer.train() ...\")\n",
    "ppo_trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
