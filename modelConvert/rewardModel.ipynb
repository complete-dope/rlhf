{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f114f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Preference / Reward model \n",
    "Here it learns to reward the \n",
    "\n",
    "** Dataset ** \n",
    "\n",
    "Prompt : Why is the color of sky blue ?\n",
    "Chosen : Its because of the scattering of blue wavelength by air molecules \n",
    "Rejected : Because sky likes blue color\n",
    "\n",
    "\n",
    "The reward model, is a linear layer at top, that learn to output whether this response is good or not \n",
    "\n",
    "Input1 to model : {Prompt + Chosen} ,  Output1 : 1  \n",
    "Input2 to model : {Prompt + Rejected}  , Output2: 0 \n",
    "\n",
    "\n",
    "Here the model learns to pick up a good response !\n",
    "\n",
    "This is the logic that happens in PPO loop ! So there is nothing as seperate training for a PPO model directly just use this as a Value head  \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# Single runnable cell — minimal, targeted fixes only (copy-paste)\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoConfig,\n",
    "    PreTrainedModel,\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from trl import RewardConfig, RewardTrainer, PreTrainedModelWrapper\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# ------------------ USER-CHOICE (preserved) ------------------\n",
    "BASE_MODEL = \"google/gemma-3-270m-it\"\n",
    "import os \n",
    "FILE_PATH = os.path.dirname(os.path.abspath(\"\"))\n",
    "# keep your FILE_PATH variable as-is in your environment\n",
    "# if it's not defined, set a sensible default (uncomment next line)\n",
    "# FILE_PATH = \"./models\"\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# 1) tokenizer (used by RewardTrainer as processing_class)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, cache_dir=FILE_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Reward model (Trained model )\n",
    "class RewardModel(PreTrainedModel):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(config)\n",
    "        # preserve your supplied model name if provided; else fall back to config\n",
    "        self.model_name = kwargs.get(\"model\", getattr(config, \"name_or_path\", BASE_MODEL))\n",
    "        # reward head => single scalar\n",
    "        self.num_labels = kwargs.get(\"num_labels\", getattr(config, \"num_labels\", 1))\n",
    "\n",
    "        # load frozen LM backbone\n",
    "        self.lm_model = AutoModelForCausalLM.from_pretrained(self.model_name, cache_dir=FILE_PATH)\n",
    "        for p in self.lm_model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # replace lm_head with scalar reward head — do NOT pass device here\n",
    "        in_features = self.lm_model.lm_head.in_features\n",
    "        self.lm_model.lm_head = nn.Linear(in_features, 1)\n",
    "        self.classifier = self.lm_model.lm_head\n",
    "        for p in self.classifier.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, return_dict: bool = True, **kwargs):\n",
    "        # pass through LM body\n",
    "        lm_out = self.lm_model.model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        last_hidden = lm_out.last_hidden_state        # [B, L, H]\n",
    "        pooled = last_hidden[:, -1, :]                # last token pooling -> [B, H]\n",
    "        rewards = self.classifier(pooled)             # [B, 1]\n",
    "        if not return_dict:\n",
    "            return (rewards,)\n",
    "        return SequenceClassifierOutput(logits=rewards)\n",
    "\n",
    "    # lightweight saving: store only head weights (as you requested earlier)\n",
    "    def save_pretrained(self, save_directory, **kwargs):\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        torch.save(self.classifier.state_dict(), os.path.join(save_directory, \"lm_head.pt\"))\n",
    "\n",
    "\n",
    "# 2) ConvertedModel: PreTrainedModel wrapper that uses frozen causal LM backbone\n",
    "\n",
    "# # PPO trainer : https://huggingface.co/docs/trl/main/en/trainer#trl.PPOTrainer\n",
    "# class ValueModel(PreTrainedModelWrapper):\n",
    "#     def __init__(self, config, **kwargs):\n",
    "#         # preserve your supplied model name if provided; else fall back to config\n",
    "#         self.model_name = kwargs.get(\"model\", getattr(config, \"name_or_path\", BASE_MODEL))\n",
    "#         # reward head => single scalar\n",
    "#         self.num_labels = kwargs.get(\"num_labels\", getattr(config, \"num_labels\", 1))\n",
    "\n",
    "#         # load frozen LM backbone\n",
    "#         lm_model = AutoModelForCausalLM.from_pretrained(self.model_name, cache_dir=FILE_PATH)\n",
    "#         for p in lm_model.parameters():\n",
    "#             p.requires_grad = False\n",
    "\n",
    "#         super().__init__(lm_model)\n",
    "#         self.lm_model = lm_model\n",
    "\n",
    "#         # replace lm_head with scalar reward head — do NOT pass device here\n",
    "#         in_features = self.lm_model.lm_head.in_features\n",
    "#         # self.lm_model.lm_head = nn.Linear(in_features, 1)\n",
    "#         self.classifier = self.lm_model.lm_head\n",
    "#         for p in self.classifier.parameters():\n",
    "#             p.requires_grad = True\n",
    "\n",
    "#     def forward(self, input_ids=None, attention_mask=None, return_dict: bool = True, **kwargs):\n",
    "#         # pass through LM body\n",
    "#         lm_out = self.lm_model.model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "#         last_hidden = lm_out.last_hidden_state        # [B, L, H]\n",
    "#         pooled = last_hidden[:, -1, :]                # last token pooling -> [B, H]\n",
    "#         rewards = self.classifier(pooled)             # [B, 1]\n",
    "#         if not return_dict:\n",
    "#             return (rewards,)\n",
    "#         return SequenceClassifierOutput(logits=rewards)\n",
    "\n",
    "#     # lightweight saving: store only head weights (as you requested earlier)\n",
    "#     def save_pretrained(self, save_directory, **kwargs):\n",
    "#         os.makedirs(save_directory, exist_ok=True)\n",
    "#         torch.save(self.classifier.state_dict(), os.path.join(save_directory, \"lm_head.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cf67778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers.modeling_outputs import ModelOutput\n",
    "from trl import PreTrainedModelWrapper\n",
    "BASE_MODEL = \"google/gemma-3-270m-it\"\n",
    "\n",
    "class ValueOutput(ModelOutput):\n",
    "    # convenient container: policy_logits (vocab logits), values (per-token scalar)\n",
    "    policy_logits: torch.Tensor  # [B, L, V]\n",
    "    values: torch.Tensor         # [B, L]\n",
    "\n",
    "class ValueModel(PreTrainedModelWrapper):\n",
    "    \"\"\"\n",
    "    Keeps lm_head (policy logits) and adds a separate value_head (per-token values).\n",
    "    \"\"\"\n",
    "    base_model_prefix = \"lm_model\"  # required for HF compatibility\n",
    "\n",
    "    def __init__(self, config, model: str =BASE_MODEL, freeze_backbone: bool = True, **kwargs):\n",
    "        num_labels = kwargs.get('num_labels', 1)\n",
    "        lm_model = AutoModelForCausalLM.from_pretrained(model, cache_dir=FILE_PATH)\n",
    "        # NOTE: we pass the model instance to the wrapper\n",
    "        super().__init__(lm_model)\n",
    "\n",
    "\n",
    "        self.lm_model = lm_model\n",
    "        self.generation_config = self.lm_model.config\n",
    "\n",
    "        # If you intend to train the policy (PPO), DON'T freeze the policy model here.\n",
    "        # But the ValueModel is typically a separate object from the policy — freeze its backbone so only\n",
    "        # the value head trains. If you are using the same model object for policy+value, manage freezing carefully.\n",
    "        if freeze_backbone:\n",
    "            for p in self.lm_model.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        hidden_size = getattr(self.lm_model.config, \"hidden_size\", None) or getattr(self.lm_model.config, \"d_model\", None)\n",
    "        if hidden_size is None:\n",
    "            raise ValueError(\"Couldn't infer hidden size from model config\")\n",
    "\n",
    "        # value head produces one scalar per token -> [B, L, 1] -> squeeze -> [B, L]\n",
    "\n",
    "        # self.value_head = nn.Sequential(\n",
    "        #     nn.Softmax(dim = -1),\n",
    "        #     self.lm_model.model.embed_tokens(), \n",
    "        #     nn.Linear(hidden_size, num_labels),\n",
    "        # )\n",
    "\n",
    "        self.value_head = nn.Linear(hidden_size, 1)\n",
    "        for p in self.value_head.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        # fallback: delegate to lm_model\n",
    "        try:\n",
    "            return super().__getattr__(name)\n",
    "        except AttributeError:\n",
    "            return getattr(self.lm_model, name)\n",
    "\n",
    "    def _get_transformer_body(self):\n",
    "        # common names; Gemma3 appears to have .model as the body\n",
    "        for attr in (\"base_model\", \"model\", \"transformer\", \"gpt_neox\", \"decoder\"):\n",
    "            candidate = getattr(self.lm_model, attr, None)\n",
    "            if candidate is not None:\n",
    "                return candidate\n",
    "        return self.lm_model\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, return_dict: bool = True, **kwargs):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          - policy_logits: LM vocab logits [B, L, V] (from lm_head applied to last_hidden_state)\n",
    "          - values: per-token scalar values [B, L]\n",
    "        Use 'policy_logits' to compute token probabilities, sample actions, compute log-probs etc.\n",
    "        \"\"\"\n",
    "        transformer = self._get_transformer_body()\n",
    "\n",
    "        # run transformer body only (avoid double heads); returns last_hidden_state [B, L, H]\n",
    "        outputs = transformer(input_ids=input_ids, attention_mask=attention_mask, return_dict=True, **kwargs)\n",
    "        last_hidden = getattr(outputs, \"last_hidden_state\", None)\n",
    "        if last_hidden is None:\n",
    "            last_hidden = outputs[0]\n",
    "\n",
    "        # 1) policy logits via the original lm_head (do NOT replace lm_head)\n",
    "        #    We call lm_head on last_hidden directly (this is equivalent to lm_model.forward's logits)\n",
    "        policy_logits = self.lm_model.lm_head(last_hidden)  # [B, L, V]\n",
    "\n",
    "        # 2) value head -> per-token scalar\n",
    "        values = self.value_head(last_hidden).squeeze(-1)  # [B, L]\n",
    "\n",
    "        if not return_dict:\n",
    "            return (policy_logits, values)\n",
    "\n",
    "        return ValueOutput(policy_logits=policy_logits, values=values)\n",
    "\n",
    "    # helper: compute chosen-token log-probs given token ids (useful for PPO loss / KL)\n",
    "    @staticmethod\n",
    "    def selected_token_logprobs(policy_logits: torch.Tensor, chosen_tokens: torch.Tensor):\n",
    "        \"\"\"\n",
    "        policy_logits: [B, L, V]\n",
    "        chosen_tokens:  [B, L]  (token ids for each position; -100 or padding for positions to ignore)\n",
    "        returns: selected_logprobs [B, L] (log-prob of the chosen token at each position)\n",
    "        \"\"\"\n",
    "        log_probs = F.log_softmax(policy_logits, dim=-1)               # [B, L, V]\n",
    "        chosen = chosen_tokens.unsqueeze(-1)                           # [B, L, 1]\n",
    "        selected_logprobs = torch.gather(log_probs, dim=-1, index=chosen).squeeze(-1)  # [B, L]\n",
    "        return selected_logprobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7537a939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the value model below this !!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3) build model from config and move to device (CPU by default)\n",
    "config = AutoConfig.from_pretrained(BASE_MODEL, num_labels=1, cache_dir=FILE_PATH)\n",
    "# print('the config is : ', config)\n",
    "conv_model = RewardModel(config, model=BASE_MODEL, num_labels=1)\n",
    "\n",
    "\n",
    "print('Initializing the value model below this !!')\n",
    "value_model= ValueModel(config , model = BASE_MODEL , num_labels=1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "727ab021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f76acc6cc4de458c80e0990082f5c8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '\\n\\nHuman: Can you describe the steps to clean fingerprints and smudges from a laptop screen\\n\\nAssistant: Yes, certainly. To clean your screen, you first need to use a microfiber cloth or soft, damp cloth to gently wipe down the surface of the screen. Next, you’ll want to grab a soft, lint-free, microfiber cleaning cloth and gently rub it back and forth across the screen to remove fingerprints and smudges.\\n\\nHuman: Can I spray isopropyl alcohol onto the cloth and clean it that way?\\n\\nAssistant:'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from trl import PPOTrainer, PPOConfig\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "# pick device\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "value_model.to(device)\n",
    "\n",
    "# 1) load dataset for PPO training\n",
    "dataset = load_dataset(\"Dahoas/rm-static\", split=\"train[:2000]\", cache_dir=FILE_PATH)\n",
    "\n",
    "# Convert into a format PPOTrainer expects\n",
    "def preprocess(examples):\n",
    "    return {\"query\": examples[\"prompt\"]}\n",
    "\n",
    "dataset = dataset.map(preprocess, remove_columns=dataset.column_names)\n",
    "\n",
    "# Now dataset is a Dataset with {\"query\": \"...\"}\n",
    "print(dataset[0])  # {'query': 'some prompt text'}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4969b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['query']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     49\u001b[39m output_max_len = \u001b[32m64\u001b[39m\n\u001b[32m     50\u001b[39m output_length_sampler = LengthSampler(output_min_len, output_max_len)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mppo_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_tensors\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mdesktop/rlhf/.venv/lib/python3.11/site-packages/accelerate/data_loader.py:567\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m     current_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    569\u001b[39m     \u001b[38;5;28mself\u001b[39m.end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mdesktop/rlhf/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mdesktop/rlhf/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mdesktop/rlhf/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mdesktop/rlhf/.venv/lib/python3.11/site-packages/transformers/data/data_collator.py:272\u001b[39m, in \u001b[36mDataCollatorWithPadding.__call__\u001b[39m\u001b[34m(self, features)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m     batch = \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m    281\u001b[39m         batch[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m] = batch[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mdesktop/rlhf/.venv/lib/python3.11/site-packages/transformers/data/data_collator.py:67\u001b[39m, in \u001b[36mpad_without_fast_tokenizer_warning\u001b[39m\u001b[34m(tokenizer, *pad_args, **pad_kwargs)\u001b[39m\n\u001b[32m     64\u001b[39m tokenizer.deprecation_warnings[\u001b[33m\"\u001b[39m\u001b[33mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     padded = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[32m     70\u001b[39m     tokenizer.deprecation_warnings[\u001b[33m\"\u001b[39m\u001b[33mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[33m\"\u001b[39m] = warning_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mdesktop/rlhf/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3347\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.pad\u001b[39m\u001b[34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[39m\n\u001b[32m   3345\u001b[39m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has been passed for padding\u001b[39;00m\n\u001b[32m   3346\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_input_names[\u001b[32m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[32m-> \u001b[39m\u001b[32m3347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3348\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3349\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model_input_names[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(encoded_inputs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   3350\u001b[39m     )\n\u001b[32m   3352\u001b[39m required_input = encoded_inputs[\u001b[38;5;28mself\u001b[39m.model_input_names[\u001b[32m0\u001b[39m]]\n\u001b[32m   3354\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m required_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(required_input, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(required_input) == \u001b[32m0\u001b[39m):\n",
      "\u001b[31mValueError\u001b[39m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['query']"
     ]
    }
   ],
   "source": [
    "from trl import PPOTrainer, PPOConfig\n",
    "import inspect\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Correctly use importlib to import PPOTrainer\n",
    "importlib.import_module('trl.trainer.ppo_trainer')\n",
    "\n",
    "# sys.modules.pop('PPOTrainer', None)\n",
    "\n",
    "\n",
    "# Print the signature of PPOTrainer in a nicely formatted way\n",
    "ppo_trainer_signature = inspect.signature(PPOTrainer)\n",
    "\n",
    "\n",
    "# 2) PPO config\n",
    "ppo_config = PPOConfig(\n",
    "    learning_rate=1.41e-5,\n",
    "    batch_size=16,\n",
    "    mini_batch_size=4,\n",
    "    gradient_accumulation_steps=1\n",
    ")\n",
    "\n",
    "# 3) reward pipeline (uses your trained conv_model as RM)\n",
    "def compute_rewards(samples, responses):\n",
    "    # samples: original prompts\n",
    "    # responses: generated model responses\n",
    "    texts = [s + r for s, r in zip(samples, responses)]\n",
    "    toks = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = conv_model(**toks).logits.squeeze(-1)\n",
    "\n",
    "    return scores  # shape: (batch,)\n",
    "\n",
    "# 4) PPO Trainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model = value_model,\n",
    "    ref_model = None, \n",
    "    args=ppo_config,\n",
    "    value_model=value_model,\n",
    "    reward_model = conv_model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# 5) main training loop\n",
    "output_min_len = 16\n",
    "output_max_len = 64\n",
    "output_length_sampler = LengthSampler(output_min_len, output_max_len)\n",
    "\n",
    "for epoch, batch in enumerate(ppo_trainer.dataloader):\n",
    "    queries = batch[\"query\"] if isinstance(batch[\"query\"], list) else [batch[\"query\"]]\n",
    "    query_tensors = tokenizer(\n",
    "        queries,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    ).input_ids.to(device)\n",
    "    # query_tensors = tokenizer(batch[\"query\"], return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "    # sample responses from the policy (actor)\n",
    "    response_tensors = ppo_trainer.generate(query_tensors, max_new_tokens=output_length_sampler())\n",
    "    responses = tokenizer.batch_decode\n",
    "    (response_tensors, skip_special_tokens=True)\n",
    "\n",
    "    # get reward scores from reward model\n",
    "    rewards = compute_rewards(batch[\"query\"], responses)\n",
    "\n",
    "    # PPO step (policy + value update)\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "print(\"✅ PPO training loop finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10dc17cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: {'query': '\\n\\nHuman: Can you describe the steps to clean fingerprints and smudges from a laptop screen\\n\\nAssistant: Yes, certainly. To clean your screen, you first need to use a microfiber cloth or soft, damp cloth to gently wipe down the surface of the screen. Next, you’ll want to grab a soft, lint-free, microfiber cleaning cloth and gently rub it back and forth across the screen to remove fingerprints and smudges.\\n\\nHuman: Can I spray isopropyl alcohol onto the cloth and clean it that way?\\n\\nAssistant:'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from trl import PPOTrainer, PPOConfig\n",
    "from transformers import AutoTokenizer\n",
    "from trl.core import LengthSampler\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\")  # or \"cuda\"\n",
    "\n",
    "# === 1) dataset: keep it as a HF Dataset and map to a single 'query' field ===\n",
    "ds = load_dataset(\"Dahoas/rm-static\", split=\"train[:2000]\", cache_dir=FILE_PATH)\n",
    "\n",
    "def keep_query(examples):\n",
    "    return {\"query\": examples[\"prompt\"]}\n",
    "\n",
    "ds = ds.map(keep_query, remove_columns=ds.column_names)  # now every row: {\"query\": \"...\"}\n",
    "# sanity\n",
    "print(\"example:\", ds[0])\n",
    "\n",
    "# === 2) tokenizer ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# === 3) collator: ALWAYS return a dict with 'query' -> list[str] ===\n",
    "def collator(batch):\n",
    "    # batch: list of dicts like [{'query': '...'}, ...]\n",
    "    queries = [b[\"query\"] for b in batch]\n",
    "    return {\"query\": queries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5542b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: {'query': '\\n\\nHuman: Can you describe the steps to clean fingerprints and smudges from a laptop screen\\n\\nAssistant: Yes, certainly. To clean your screen, you first need to use a microfiber cloth or soft, damp cloth to gently wipe down the surface of the screen. Next, you’ll want to grab a soft, lint-free, microfiber cleaning cloth and gently rub it back and forth across the screen to remove fingerprints and smudges.\\n\\nHuman: Can I spray isopropyl alcohol onto the cloth and clean it that way?\\n\\nAssistant:'}\n",
      "starting the training loop here \n",
      "===training policy===\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     71\u001b[39m     ppo_trainer.reward_fn = compute_rewards\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# Now call push-button train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[43mppo_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# this should run the full PPO pipeline: generate → reward → step → update\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mdesktop/rlhf/.venv/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:416\u001b[39m, in \u001b[36mPPOTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    414\u001b[39m data = \u001b[38;5;28mnext\u001b[39m(iter_dataloader)\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     queries = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m(device)\n\u001b[32m    417\u001b[39m     context_length = queries.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    418\u001b[39m     responses = []\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# === 4) instantiate (make sure ref_model exists if your constructor requires it) ===\n",
    "# if your PPOTrainer signature expects args first (like your custom class), adapt accordingly.\n",
    "# Typical TRL usage:\n",
    "ref_model = copy.deepcopy(value_model)\n",
    "ref_model.to(device)\n",
    "ref_model.eval()\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    args=ppo_config,                # or config=ppo_config depending on your class\n",
    "    processing_class=tokenizer,     # some custom class uses this name\n",
    "    model=value_model,\n",
    "    value_model=value_model,\n",
    "    ref_model=ref_model,            # provide a frozen reference model if required\n",
    "    reward_model=conv_model,\n",
    "    train_dataset=ds,               # pass the HF Dataset, NOT ds['query']\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "# === 5) reward function: conv_model returns scalar per sequence ===\n",
    "def compute_rewards(queries, responses):\n",
    "    # queries: list[str], responses: list[str]\n",
    "    texts = [q + r for q, r in zip(queries, responses)]\n",
    "    toks = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        scores = conv_model(input_ids=toks[\"input_ids\"], attention_mask=toks[\"attention_mask\"]).logits\n",
    "        # squeeze if shape [B, L, 1] or [B,1]\n",
    "        scores = scores.squeeze()\n",
    "        if scores.dim() == 2:  # shape [B, L] -> reduce if needed (use final token or sum)\n",
    "            scores = scores[:, -1]  \n",
    "    return scores.detach().cpu()\n",
    "\n",
    "# === 6) training loop: be defensive about batch types ===\n",
    "\n",
    "print(\"starting the training loop here \")\n",
    "output_min_len, output_max_len = 16, 64\n",
    "output_length_sampler = LengthSampler(output_min_len, output_max_len)\n",
    "\n",
    "if not hasattr(ppo_trainer, \"reward_fn\"):\n",
    "    ppo_trainer.reward_fn = compute_rewards\n",
    "else:\n",
    "    ppo_trainer.reward_fn = compute_rewards\n",
    "\n",
    "# Now call push-button train\n",
    "ppo_trainer.train()   # this should run the full PPO pipeline: generate → reward → step → update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55c10fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacity of 7.63 GiB of which 516.44 MiB is free. Including non-PyTorch memory, this process has 7.10 GiB memory in use. Of the allocated memory 6.99 GiB is allocated by PyTorch, and 8.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m conv_model.to(device)\n\u001b[32m     12\u001b[39m conv_model.eval()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m ref_model = \u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m ref_model.to(device)\n\u001b[32m     15\u001b[39m ref_model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:172\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    170\u001b[39m                 y = x\n\u001b[32m    171\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m                 y = \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:271\u001b[39m, in \u001b[36m_reconstruct\u001b[39m\u001b[34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m         state = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[33m'\u001b[39m\u001b[33m__setstate__\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    273\u001b[39m         y.__setstate__(state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:146\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    144\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:231\u001b[39m, in \u001b[36m_deepcopy_dict\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    229\u001b[39m memo[\u001b[38;5;28mid\u001b[39m(x)] = y\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     y[deepcopy(key, memo)] = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:146\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    144\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:231\u001b[39m, in \u001b[36m_deepcopy_dict\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    229\u001b[39m memo[\u001b[38;5;28mid\u001b[39m(x)] = y\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     y[deepcopy(key, memo)] = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:172\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    170\u001b[39m                 y = x\n\u001b[32m    171\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m                 y = \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:271\u001b[39m, in \u001b[36m_reconstruct\u001b[39m\u001b[34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m         state = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[33m'\u001b[39m\u001b[33m__setstate__\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    273\u001b[39m         y.__setstate__(state)\n",
      "    \u001b[31m[... skipping similar frames: _deepcopy_dict at line 231 (4 times), deepcopy at line 146 (4 times), _reconstruct at line 271 (1 times), deepcopy at line 172 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:172\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    170\u001b[39m                 y = x\n\u001b[32m    171\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m                 y = \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:271\u001b[39m, in \u001b[36m_reconstruct\u001b[39m\u001b[34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m         state = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[33m'\u001b[39m\u001b[33m__setstate__\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    273\u001b[39m         y.__setstate__(state)\n",
      "    \u001b[31m[... skipping similar frames: _deepcopy_dict at line 231 (1 times), deepcopy at line 146 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:146\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    144\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:231\u001b[39m, in \u001b[36m_deepcopy_dict\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    229\u001b[39m memo[\u001b[38;5;28mid\u001b[39m(x)] = y\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     y[deepcopy(key, memo)] = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/copy.py:153\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    151\u001b[39m copier = \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33m__deepcopy__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    155\u001b[39m     reductor = dispatch_table.get(\u001b[38;5;28mcls\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mdesktop/rlhf/.venv/lib/python3.11/site-packages/torch/nn/parameter.py:68\u001b[39m, in \u001b[36mParameter.__deepcopy__\u001b[39m\u001b[34m(self, memo)\u001b[39m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)]\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     67\u001b[39m     result = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.requires_grad\n\u001b[32m     69\u001b[39m     )\n\u001b[32m     70\u001b[39m     memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)] = result\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacity of 7.63 GiB of which 516.44 MiB is free. Including non-PyTorch memory, this process has 7.10 GiB memory in use. Of the allocated memory 6.99 GiB is allocated by PyTorch, and 8.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import copy, inspect, torch\n",
    "from trl import PPOTrainer\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "# device, dataset, tokenizer, value_model, conv_model, ppo_config must already exist\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() and torch.device(\"mps\") else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# quick safety: move models to device\n",
    "value_model.to(device)\n",
    "conv_model.to(device)\n",
    "conv_model.eval()\n",
    "ref_model = copy.deepcopy(value_model)\n",
    "ref_model.to(device)\n",
    "ref_model.eval()\n",
    "\n",
    "# make sure ds is a HF Dataset with 'query' column\n",
    "print(\"example dataset row:\", ds[0])\n",
    "\n",
    "# collator that returns list[str] under 'query'\n",
    "def collator(batch):\n",
    "    return {\"query\": [b[\"query\"] for b in batch]}\n",
    "\n",
    "# inspect signature and build kwargs\n",
    "sig = inspect.signature(PPOTrainer.__init__)\n",
    "params = list(sig.parameters.keys())\n",
    "print(\"PPOTrainer.__init__ params:\", params)\n",
    "\n",
    "kw = {}\n",
    "# config / args\n",
    "if \"config\" in params:\n",
    "    kw[\"config\"] = ppo_config\n",
    "elif \"args\" in params:\n",
    "    kw[\"args\"] = ppo_config\n",
    "else:\n",
    "    raise RuntimeError(\"Can't find 'config' or 'args' in PPOTrainer signature; inspect and adapt.\")\n",
    "\n",
    "# tokenizer / processing_class\n",
    "if \"tokenizer\" in params:\n",
    "    kw[\"tokenizer\"] = tokenizer\n",
    "elif \"processing_class\" in params:\n",
    "    kw[\"processing_class\"] = tokenizer\n",
    "\n",
    "# required models\n",
    "if \"model\" in params:\n",
    "    kw[\"model\"] = value_model  # often the actor/policy (if your trainer expects a separate value_model, we set that below)\n",
    "if \"ref_model\" in params:\n",
    "    kw[\"ref_model\"] = ref_model\n",
    "if \"reward_model\" in params:\n",
    "    kw[\"reward_model\"] = conv_model\n",
    "# IMPORTANT: some local/trl forks expect a separate 'value_model' parameter\n",
    "if \"value_model\" in params:\n",
    "    kw[\"value_model\"] = value_model\n",
    "\n",
    "# dataset param name\n",
    "if \"dataset\" in params:\n",
    "    kw[\"dataset\"] = ds\n",
    "elif \"train_dataset\" in params:\n",
    "    kw[\"train_dataset\"] = ds\n",
    "\n",
    "# data collator\n",
    "if \"data_collator\" in params:\n",
    "    kw[\"data_collator\"] = collator\n",
    "\n",
    "print(\"Instantiating PPOTrainer with kwargs:\", list(kw.keys()))\n",
    "ppo_trainer = PPOTrainer(**kw)\n",
    "\n",
    "# attach a reward function (safe)\n",
    "def compute_rewards(queries, responses):\n",
    "    texts = [q + r for q, r in zip(queries, responses)]\n",
    "    toks = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = conv_model(input_ids=toks[\"input_ids\"], attention_mask=toks[\"attention_mask\"])\n",
    "    scores = getattr(out, \"logits\", out)\n",
    "    scores = torch.as_tensor(scores).squeeze()\n",
    "    if scores.dim() == 2:\n",
    "        if scores.shape[1] == 1:\n",
    "            scores = scores[:, 0]\n",
    "        else:\n",
    "            scores = scores[:, -1]\n",
    "    return scores.cpu()\n",
    "\n",
    "ppo_trainer.reward_fn = compute_rewards\n",
    "\n",
    "print(\"Calling ppo_trainer.train() ...\")\n",
    "ppo_trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
