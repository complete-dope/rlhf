{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd8c6578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import os \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21c4ca6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path :  .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert Einstein's fundamental laws, like the photoelectric effect and the Einstein field equations, describe the nature of reality and the fundamental processes of the universe.\n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "FILE_PATH = os.path.curdir\n",
    "# FILE_PATH = os.path.dirname(__file__)\n",
    "\n",
    "print(\"File path : \", FILE_PATH)\n",
    "\n",
    "BASE_MODEL = \"google/gemma-3-270m-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, cache_dir = FILE_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, cache_dir = FILE_PATH)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Act like albert einstein and explain me faraday laws in 30 words\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf91994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c284e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75652c26e4244ffbb9c094b2360c016f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.gemma3.configuration_gemma3.Gemma3TextConfig'> for this kind of AutoModel: AutoModelForSequenceClassification.\nModel type should be one of AlbertConfig, ArceeConfig, BartConfig, BertConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BloomConfig, CamembertConfig, CanineConfig, LlamaConfig, ConvBertConfig, CTRLConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DeepseekV2Config, DeepseekV3Config, DiffLlamaConfig, DistilBertConfig, DogeConfig, ElectraConfig, ErnieConfig, ErnieMConfig, EsmConfig, Exaone4Config, FalconConfig, FlaubertConfig, FNetConfig, FunnelConfig, GemmaConfig, Gemma2Config, Gemma3Config, GlmConfig, Glm4Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GptOssConfig, GPTJConfig, HeliumConfig, HunYuanDenseV1Config, HunYuanMoEV1Config, IBertConfig, JambaConfig, JetMoeConfig, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LiltConfig, LlamaConfig, LongformerConfig, LukeConfig, MarkupLMConfig, MBartConfig, MegaConfig, MegatronBertConfig, MiniMaxConfig, MistralConfig, MixtralConfig, MobileBertConfig, ModernBertConfig, ModernBertDecoderConfig, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NemotronConfig, NezhaConfig, NystromformerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PerceiverConfig, PersimmonConfig, PhiConfig, Phi3Config, PhimoeConfig, PLBartConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SeedOssConfig, SmolLM3Config, SqueezeBertConfig, StableLmConfig, Starcoder2Config, T5Config, T5GemmaConfig, TapasConfig, TransfoXLConfig, UMT5Config, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YosoConfig, ZambaConfig, Zamba2Config.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# model here is a causual model \u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# convert that model to sequence classifier\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForSequenceClassification\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model_sc = \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_MODEL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m inputs = tokenizer.apply_chat_template(\n\u001b[32m     10\u001b[39m     messages,\n\u001b[32m     11\u001b[39m     add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m ).to(model.device)\n\u001b[32m     17\u001b[39m outputs = model.generate(**inputs, max_new_tokens=\u001b[32m200\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/personal/rlhf/.venv/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:607\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m    604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class.from_pretrained(\n\u001b[32m    605\u001b[39m         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n\u001b[32m    606\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Unrecognized configuration class <class 'transformers.models.gemma3.configuration_gemma3.Gemma3TextConfig'> for this kind of AutoModel: AutoModelForSequenceClassification.\nModel type should be one of AlbertConfig, ArceeConfig, BartConfig, BertConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BloomConfig, CamembertConfig, CanineConfig, LlamaConfig, ConvBertConfig, CTRLConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DeepseekV2Config, DeepseekV3Config, DiffLlamaConfig, DistilBertConfig, DogeConfig, ElectraConfig, ErnieConfig, ErnieMConfig, EsmConfig, Exaone4Config, FalconConfig, FlaubertConfig, FNetConfig, FunnelConfig, GemmaConfig, Gemma2Config, Gemma3Config, GlmConfig, Glm4Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GptOssConfig, GPTJConfig, HeliumConfig, HunYuanDenseV1Config, HunYuanMoEV1Config, IBertConfig, JambaConfig, JetMoeConfig, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LiltConfig, LlamaConfig, LongformerConfig, LukeConfig, MarkupLMConfig, MBartConfig, MegaConfig, MegatronBertConfig, MiniMaxConfig, MistralConfig, MixtralConfig, MobileBertConfig, ModernBertConfig, ModernBertDecoderConfig, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NemotronConfig, NezhaConfig, NystromformerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PerceiverConfig, PersimmonConfig, PhiConfig, Phi3Config, PhimoeConfig, PLBartConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SeedOssConfig, SmolLM3Config, SqueezeBertConfig, StableLmConfig, Starcoder2Config, T5Config, T5GemmaConfig, TapasConfig, TransfoXLConfig, UMT5Config, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YosoConfig, ZambaConfig, Zamba2Config."
     ]
    }
   ],
   "source": [
    "# model here is a causual model \n",
    "\n",
    "# convert that model to sequence classifier\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_sc = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL)\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e15819",
   "metadata": {},
   "source": [
    "https://github.com/RLHFlow/RLHF-Reward-Modeling/blob/main/bradley-terry-rm/gemma_2B_rm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7bb85c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b6ab673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a411d777c64954ae9d7169d31867fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GemmaForSequenceClassification were not initialized from the model checkpoint at google/gemma-2b-it and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded  GemmaForSequenceClassification(\n",
      "  (model): GemmaModel(\n",
      "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-17): 18 x GemmaDecoderLayer(\n",
      "        (self_attn): GemmaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): GemmaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "          (act_fn): GELUActivation()\n",
      "        )\n",
      "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): GemmaRotaryEmbedding()\n",
      "  )\n",
      "  (score): Linear(in_features=2048, out_features=1, bias=False)\n",
      ")\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohitdulani/Desktop/personal/rlhf/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# This script is modified from the TRL package https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama/scripts/reward_modeling.py\n",
    "# This script is designed for the reward modeling with Gemma model but can also be applied to any models with a chat template and an official pad token\n",
    "# If you have any question, feel free to send me an email via wx13@illinois.edu\n",
    "########################\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "# import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "# from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    PreTrainedTokenizerBase,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.utils import PaddingStrategy\n",
    "\n",
    "import pdb\n",
    "\n",
    "\n",
    "# Define and parse arguments.\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    \"\"\"\n",
    "    These arguments vary depending on how many GPUs you have, what their capacity and features are, and what size model you want to train.\n",
    "    \"\"\"\n",
    "    local_rank: Optional[int] = field(\n",
    "        default=-1, metadata={\"help\": \"Used for multi-gpu\"})\n",
    "\n",
    "    deepspeed: Optional[str] = field(\n",
    "        # default=\"dp3.json\",\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Path to deepspeed config if using deepspeed. You may need this if the model that you want to train doesn't fit on a single GPU.\"\n",
    "        },\n",
    "    )\n",
    "    per_device_train_batch_size: Optional[int] = field(default=1)\n",
    "    per_device_eval_batch_size: Optional[int] = field(default=1)\n",
    "    gradient_accumulation_steps: Optional[int] = field(default=32)\n",
    "    learning_rate: Optional[float] = field(default=1e-5)\n",
    "    weight_decay: Optional[float] = field(default=0.001)\n",
    "    model_name: Optional[str] = field(\n",
    "        default=\"google/gemma-2b-it\", #\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        metadata={\n",
    "            \"help\": \"The model that you want to train from the Hugging Face hub. E.g. gpt2, gpt2-xl, bert, etc.\"\n",
    "        },\n",
    "    )\n",
    "    bf16: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"This essentially cuts the training time in half if you want to sacrifice a little precision and have a supported GPU.\"\n",
    "        },\n",
    "    )\n",
    "    num_train_epochs: Optional[int] = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"The number of training epochs for the reward model.\"},\n",
    "    )\n",
    "    train_set_path: Optional[str] = field(\n",
    "        default=\"hendrydong/preference_700K\",\n",
    "        metadata={\"help\": \"The dir of the subset of the training data to use\"},\n",
    "    )\n",
    "    eval_set_path: Optional[str] = field(\n",
    "        default=\"hendrydong/preference_700K\",\n",
    "        metadata={\"help\": \"The dir of the subset of the eval data to use\"},\n",
    "    )\n",
    "    output_path: Optional[str] = field(\n",
    "        default=\"./bt_models/gemma2b_rm\",\n",
    "        metadata={\"help\": \"The dir for output model\"},\n",
    "    )\n",
    "    gradient_checkpointing: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Enables gradient checkpointing.\"},\n",
    "    )\n",
    "    optim: Optional[str] = field(\n",
    "        # default=\"adamw_hf\",\n",
    "        default=\"paged_adamw_32bit\",\n",
    "        # default=\"adamw_torch_fused\",\n",
    "        metadata={\"help\": \"The optimizer to use.\"},\n",
    "    )\n",
    "    lr_scheduler_type: Optional[str] = field(\n",
    "        default=\"cosine\",\n",
    "        metadata={\"help\": \"The lr scheduler\"},\n",
    "    )\n",
    "    max_length: Optional[int] = field(default=4096)\n",
    "\n",
    "    save_every_steps: Optional[int] = field(\n",
    "        default=999999,\n",
    "        metadata={\"help\": \"Save the model every x steps\"},\n",
    "    )\n",
    "    eval_every_steps: Optional[int] = field(\n",
    "        default=999999,\n",
    "        metadata={\"help\": \"Eval the model every x steps\"},\n",
    "    )\n",
    "\n",
    "parser = HfArgumentParser(ScriptArguments)\n",
    "\n",
    "script_args = parser.parse_args_into_dataclasses(args=[])[0]\n",
    "\n",
    "# # Load the value-head model and tokenizer.\n",
    "# tokenizer_name = script_args.model_name\n",
    "# tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_auth_token=True)\n",
    "\n",
    "# # Adjusted according to the base model\n",
    "# # Need to do this for the models that don't have an official pad token.\n",
    "# tokenizer.truncation_side = \"left\"\n",
    "# tokenizer.model_max_length = script_args.max_length\n",
    "\n",
    "# Get the dataset\n",
    "# train_path = script_args.train_set_path\n",
    "# eval_path = script_args.eval_set_path\n",
    "# output_name = script_args.output_path\n",
    "\n",
    "def build_dataset(tokenizer, train_path, eval_path):\n",
    "\n",
    "    def tokenize(sample):\n",
    "        \n",
    "        sample['positive'] = tokenizer.apply_chat_template(\n",
    "            sample['chosen'], tokenize=False, add_generation_prompt=False).replace(tokenizer.bos_token, \"\")\n",
    "        sample['negative'] = tokenizer.apply_chat_template(\n",
    "            sample['rejected'], tokenize=False, add_generation_prompt=False).replace(tokenizer.bos_token, \"\")\n",
    "        \n",
    "        tokenized_pos = tokenizer(sample['positive'], truncation=True)\n",
    "        tokenized_neg = tokenizer(sample['negative'], truncation=True)\n",
    "        sample[\"input_ids_j\"] = tokenized_pos[\"input_ids\"]\n",
    "        sample[\"attention_mask_j\"] = tokenized_pos[\"attention_mask\"]\n",
    "        sample[\"input_ids_k\"] = tokenized_neg[\"input_ids\"]\n",
    "        sample[\"attention_mask_k\"] = tokenized_neg[\"attention_mask\"]\n",
    "        return sample\n",
    "    \n",
    "    ds = load_dataset(train_path, split=\"train\").shuffle(seed=42)\n",
    "    #ds = ds.select(range(2000))\n",
    "    ds = ds.map(tokenize, num_proc=8)\n",
    "\n",
    "    eval_dataset = None\n",
    "\n",
    "    train_dataset = ds\n",
    "    eval_dataset = load_dataset(eval_path, split=\"train\").shuffle(seed=42).select(range(500))\n",
    "    #eval_dataset = ds.select(range(500))\n",
    "    return train_dataset, eval_dataset\n",
    "\n",
    "\n",
    "# train_dataset, eval_dataset = build_dataset(tokenizer, train_path, eval_path)\n",
    "# print(\"Training set: \", len(train_dataset), \" Eval set: \", len(eval_dataset))\n",
    "\n",
    "# Define the trainer\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=output_name,\n",
    "#     learning_rate=script_args.learning_rate,\n",
    "#     per_device_train_batch_size=script_args.per_device_train_batch_size,\n",
    "#     per_device_eval_batch_size=script_args.per_device_eval_batch_size,\n",
    "#     num_train_epochs=script_args.num_train_epochs,\n",
    "#     weight_decay=script_args.weight_decay,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     eval_steps=script_args.eval_every_steps,\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=script_args.save_every_steps,\n",
    "#     gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    "#     gradient_checkpointing=script_args.gradient_checkpointing,\n",
    "#     deepspeed=script_args.deepspeed,\n",
    "#     local_rank=script_args.local_rank,\n",
    "#     remove_unused_columns=False,\n",
    "#     label_names=[],\n",
    "#     bf16=script_args.bf16,\n",
    "#     logging_strategy=\"steps\",\n",
    "#     logging_steps=10,\n",
    "#     optim=script_args.optim,\n",
    "#     lr_scheduler_type=script_args.lr_scheduler_type,\n",
    "#     warmup_ratio=0.03,\n",
    "#     report_to='wandb'\n",
    "# )\n",
    "\n",
    "# enable if you want to train with lora\n",
    "# peft_config = LoraConfig(\n",
    "#     task_type=TaskType.SEQ_CLS,\n",
    "#     inference_mode=False,\n",
    "#     r=8,\n",
    "#     lora_alpha=32,\n",
    "#     lora_dropout=0.1,\n",
    "# )\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    script_args.model_name, num_labels=1, torch_dtype=torch.bfloat16, cache_dir = FILE_PATH\n",
    ")\n",
    "# model = get_peft_model(model, peft_config)\n",
    "# model.print_trainable_parameters()\n",
    "print(\"model loaded \", model)\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.exit(0)\n",
    "\n",
    "model.config.use_cache = not script_args.gradient_checkpointing\n",
    "num_proc = 24  # Can adjust to be higher if you have more processors.\n",
    "original_columns = train_dataset.column_names\n",
    "\n",
    "\n",
    "# We need to define a special data collator that batches the data in our j vs k format.\n",
    "@dataclass\n",
    "class RewardDataCollatorWithPadding:\n",
    "    tokenizer: AutoTokenizer\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        merged_features = []\n",
    "        for feature in features:\n",
    "            merged_features.append(\n",
    "                {\n",
    "                    \"input_ids\": feature[\"input_ids_j\"],\n",
    "                    \"attention_mask\": feature[\"attention_mask_j\"],\n",
    "                }\n",
    "            )\n",
    "            merged_features.append(\n",
    "                {\n",
    "                    \"input_ids\": feature[\"input_ids_k\"],\n",
    "                    \"attention_mask\": feature[\"attention_mask_k\"],\n",
    "                }\n",
    "            )\n",
    "        batch = self.tokenizer.pad(\n",
    "            merged_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        batch = {\n",
    "            \"input_ids\": batch[\"input_ids\"],\n",
    "            \"attention_mask\": batch[\"attention_mask\"],\n",
    "            \"return_loss\": True,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "\n",
    "# Define the trainer\n",
    "def compute_metrics(eval_pred):\n",
    "    result = {}\n",
    "    pos_predictions_scores = eval_pred.predictions[0]\n",
    "    neg_predictions_scores = eval_pred.predictions[1]\n",
    "    # We assume that the first sample is preferred by default in groundtruth\n",
    "    result['accuracy'] = np.sum(\n",
    "        pos_predictions_scores > neg_predictions_scores) / len(pos_predictions_scores)\n",
    "    return result\n",
    "\n",
    "\n",
    "class RewardTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        rewards = model(\n",
    "            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n",
    "        )[0]\n",
    "        bsz = rewards.size(0)\n",
    "        jidx = torch.arange(0, bsz, 2)\n",
    "        kidx = jidx + 1\n",
    "        rewards_j = rewards[jidx]\n",
    "        rewards_k = rewards[kidx]\n",
    "        loss = -nn.functional.logsigmoid(rewards_j - rewards_k).mean()\n",
    "        if return_outputs:\n",
    "            return loss, {\"rewards_j\": rewards_j, \"rewards_k\": rewards_k}\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Train the model, woohoo.\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=RewardDataCollatorWithPadding(\n",
    "        tokenizer=tokenizer, max_length=script_args.max_length),\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "print(\"Saving last checkpoint of the model\")\n",
    "#model.save_pretrained(output_name + \"/last_checkpoint\")\n",
    "trainer.save_model(output_name + \"/last_checkpoint\")\n",
    "tokenizer.save_pretrained(output_name + \"/last_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883e2744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900931d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdac61f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "------\n",
      "------\n",
      "------\n",
      "script _args are  google/gemma-3-270m-it\n",
      "------\n",
      "------\n",
      "------\n",
      "------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.gemma3.configuration_gemma3.Gemma3TextConfig'> for this kind of AutoModel: AutoModelForSequenceClassification.\nModel type should be one of AlbertConfig, ArceeConfig, BartConfig, BertConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BloomConfig, CamembertConfig, CanineConfig, LlamaConfig, ConvBertConfig, CTRLConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DeepseekV2Config, DeepseekV3Config, DiffLlamaConfig, DistilBertConfig, DogeConfig, ElectraConfig, ErnieConfig, ErnieMConfig, EsmConfig, Exaone4Config, FalconConfig, FlaubertConfig, FNetConfig, FunnelConfig, GemmaConfig, Gemma2Config, Gemma3Config, GlmConfig, Glm4Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GptOssConfig, GPTJConfig, HeliumConfig, HunYuanDenseV1Config, HunYuanMoEV1Config, IBertConfig, JambaConfig, JetMoeConfig, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LiltConfig, LlamaConfig, LongformerConfig, LukeConfig, MarkupLMConfig, MBartConfig, MegaConfig, MegatronBertConfig, MiniMaxConfig, MistralConfig, MixtralConfig, MobileBertConfig, ModernBertConfig, ModernBertDecoderConfig, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NemotronConfig, NezhaConfig, NystromformerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PerceiverConfig, PersimmonConfig, PhiConfig, Phi3Config, PhimoeConfig, PLBartConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SeedOssConfig, SmolLM3Config, SqueezeBertConfig, StableLmConfig, Starcoder2Config, T5Config, T5GemmaConfig, TapasConfig, TransfoXLConfig, UMT5Config, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YosoConfig, ZambaConfig, Zamba2Config.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 196\u001b[39m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m train_dataset, eval_dataset\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# train_dataset, eval_dataset = build_dataset(tokenizer, train_path, eval_path)\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m# print(\"Training set: \", len(train_dataset), \" Eval set: \", len(eval_dataset))\u001b[39;00m\n\u001b[32m    159\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    193\u001b[39m \u001b[38;5;66;03m#     lora_dropout=0.1,\u001b[39;00m\n\u001b[32m    194\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m model = \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscript_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mFILE_PATH\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# model = get_peft_model(model, peft_config)\u001b[39;00m\n\u001b[32m    200\u001b[39m \u001b[38;5;66;03m# model.print_trainable_parameters()\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mmodel loaded \u001b[39m\u001b[33m\"\u001b[39m, model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/personal/rlhf/.venv/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:607\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m    604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class.from_pretrained(\n\u001b[32m    605\u001b[39m         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n\u001b[32m    606\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Unrecognized configuration class <class 'transformers.models.gemma3.configuration_gemma3.Gemma3TextConfig'> for this kind of AutoModel: AutoModelForSequenceClassification.\nModel type should be one of AlbertConfig, ArceeConfig, BartConfig, BertConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BloomConfig, CamembertConfig, CanineConfig, LlamaConfig, ConvBertConfig, CTRLConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DeepseekV2Config, DeepseekV3Config, DiffLlamaConfig, DistilBertConfig, DogeConfig, ElectraConfig, ErnieConfig, ErnieMConfig, EsmConfig, Exaone4Config, FalconConfig, FlaubertConfig, FNetConfig, FunnelConfig, GemmaConfig, Gemma2Config, Gemma3Config, GlmConfig, Glm4Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GptOssConfig, GPTJConfig, HeliumConfig, HunYuanDenseV1Config, HunYuanMoEV1Config, IBertConfig, JambaConfig, JetMoeConfig, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LiltConfig, LlamaConfig, LongformerConfig, LukeConfig, MarkupLMConfig, MBartConfig, MegaConfig, MegatronBertConfig, MiniMaxConfig, MistralConfig, MixtralConfig, MobileBertConfig, ModernBertConfig, ModernBertDecoderConfig, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NemotronConfig, NezhaConfig, NystromformerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PerceiverConfig, PersimmonConfig, PhiConfig, Phi3Config, PhimoeConfig, PLBartConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SeedOssConfig, SmolLM3Config, SqueezeBertConfig, StableLmConfig, Starcoder2Config, T5Config, T5GemmaConfig, TapasConfig, TransfoXLConfig, UMT5Config, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YosoConfig, ZambaConfig, Zamba2Config."
     ]
    }
   ],
   "source": [
    "########################\n",
    "# This script is modified from the TRL package https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama/scripts/reward_modeling.py\n",
    "# This script is designed for the reward modeling with Gemma model but can also be applied to any models with a chat template and an official pad token\n",
    "# If you have any question, feel free to send me an email via wx13@illinois.edu\n",
    "########################\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "# import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "# from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    PreTrainedTokenizerBase,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.utils import PaddingStrategy\n",
    "\n",
    "import pdb\n",
    "\n",
    "\n",
    "# Define and parse arguments.\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    \"\"\"\n",
    "    These arguments vary depending on how many GPUs you have, what their capacity and features are, and what size model you want to train.\n",
    "    \"\"\"\n",
    "    local_rank: Optional[int] = field(\n",
    "        default=-1, metadata={\"help\": \"Used for multi-gpu\"})\n",
    "\n",
    "    deepspeed: Optional[str] = field(\n",
    "        # default=\"dp3.json\",\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Path to deepspeed config if using deepspeed. You may need this if the model that you want to train doesn't fit on a single GPU.\"\n",
    "        },\n",
    "    )\n",
    "    per_device_train_batch_size: Optional[int] = field(default=1)\n",
    "    per_device_eval_batch_size: Optional[int] = field(default=1)\n",
    "    gradient_accumulation_steps: Optional[int] = field(default=32)\n",
    "    learning_rate: Optional[float] = field(default=1e-5)\n",
    "    weight_decay: Optional[float] = field(default=0.001)\n",
    "    model_name: Optional[str] = field(\n",
    "        default=\"google/gemma-3-270m-it\", #\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        metadata={\n",
    "            \"help\": \"The model that you want to train from the Hugging Face hub. E.g. gpt2, gpt2-xl, bert, etc.\"\n",
    "        },\n",
    "    )\n",
    "    bf16: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"This essentially cuts the training time in half if you want to sacrifice a little precision and have a supported GPU.\"\n",
    "        },\n",
    "    )\n",
    "    num_train_epochs: Optional[int] = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"The number of training epochs for the reward model.\"},\n",
    "    )\n",
    "    train_set_path: Optional[str] = field(\n",
    "        default=\"hendrydong/preference_700K\",\n",
    "        metadata={\"help\": \"The dir of the subset of the training data to use\"},\n",
    "    )\n",
    "    eval_set_path: Optional[str] = field(\n",
    "        default=\"hendrydong/preference_700K\",\n",
    "        metadata={\"help\": \"The dir of the subset of the eval data to use\"},\n",
    "    )\n",
    "    output_path: Optional[str] = field(\n",
    "        default=\"./bt_models/gemma2b_rm\",\n",
    "        metadata={\"help\": \"The dir for output model\"},\n",
    "    )\n",
    "    gradient_checkpointing: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Enables gradient checkpointing.\"},\n",
    "    )\n",
    "    optim: Optional[str] = field(\n",
    "        # default=\"adamw_hf\",\n",
    "        default=\"paged_adamw_32bit\",\n",
    "        # default=\"adamw_torch_fused\",\n",
    "        metadata={\"help\": \"The optimizer to use.\"},\n",
    "    )\n",
    "    lr_scheduler_type: Optional[str] = field(\n",
    "        default=\"cosine\",\n",
    "        metadata={\"help\": \"The lr scheduler\"},\n",
    "    )\n",
    "    max_length: Optional[int] = field(default=4096)\n",
    "\n",
    "    save_every_steps: Optional[int] = field(\n",
    "        default=999999,\n",
    "        metadata={\"help\": \"Save the model every x steps\"},\n",
    "    )\n",
    "    eval_every_steps: Optional[int] = field(\n",
    "        default=999999,\n",
    "        metadata={\"help\": \"Eval the model every x steps\"},\n",
    "    )\n",
    "\n",
    "parser = HfArgumentParser(ScriptArguments)\n",
    "\n",
    "script_args = parser.parse_args_into_dataclasses(args=[])[0]\n",
    "\n",
    "print(\"------\\n------\")\n",
    "print(\"------\\n------\")\n",
    "print(\"script _args are \",script_args.model_name)\n",
    "print(\"------\\n------\")\n",
    "print(\"------\\n------\")\n",
    "\n",
    "\n",
    "# # Load the value-head model and tokenizer.\n",
    "# tokenizer_name = script_args.model_name\n",
    "# tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_auth_token=True)\n",
    "\n",
    "# # Adjusted according to the base model\n",
    "# # Need to do this for the models that don't have an official pad token.\n",
    "# tokenizer.truncation_side = \"left\"\n",
    "# tokenizer.model_max_length = script_args.max_length\n",
    "\n",
    "# Get the dataset\n",
    "# train_path = script_args.train_set_path\n",
    "# eval_path = script_args.eval_set_path\n",
    "# output_name = script_args.output_path\n",
    "\n",
    "def build_dataset(tokenizer, train_path, eval_path):\n",
    "\n",
    "    def tokenize(sample):\n",
    "        \n",
    "        sample['positive'] = tokenizer.apply_chat_template(\n",
    "            sample['chosen'], tokenize=False, add_generation_prompt=False).replace(tokenizer.bos_token, \"\")\n",
    "        sample['negative'] = tokenizer.apply_chat_template(\n",
    "            sample['rejected'], tokenize=False, add_generation_prompt=False).replace(tokenizer.bos_token, \"\")\n",
    "        \n",
    "        tokenized_pos = tokenizer(sample['positive'], truncation=True)\n",
    "        tokenized_neg = tokenizer(sample['negative'], truncation=True)\n",
    "        sample[\"input_ids_j\"] = tokenized_pos[\"input_ids\"]\n",
    "        sample[\"attention_mask_j\"] = tokenized_pos[\"attention_mask\"]\n",
    "        sample[\"input_ids_k\"] = tokenized_neg[\"input_ids\"]\n",
    "        sample[\"attention_mask_k\"] = tokenized_neg[\"attention_mask\"]\n",
    "        return sample\n",
    "    \n",
    "    ds = load_dataset(train_path, split=\"train\").shuffle(seed=42)\n",
    "    #ds = ds.select(range(2000))\n",
    "    ds = ds.map(tokenize, num_proc=8)\n",
    "\n",
    "    eval_dataset = None\n",
    "\n",
    "    train_dataset = ds\n",
    "    eval_dataset = load_dataset(eval_path, split=\"train\").shuffle(seed=42).select(range(500))\n",
    "    #eval_dataset = ds.select(range(500))\n",
    "    return train_dataset, eval_dataset\n",
    "\n",
    "\n",
    "# train_dataset, eval_dataset = build_dataset(tokenizer, train_path, eval_path)\n",
    "# print(\"Training set: \", len(train_dataset), \" Eval set: \", len(eval_dataset))\n",
    "\n",
    "# Define the trainer\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=output_name,\n",
    "#     learning_rate=script_args.learning_rate,\n",
    "#     per_device_train_batch_size=script_args.per_device_train_batch_size,\n",
    "#     per_device_eval_batch_size=script_args.per_device_eval_batch_size,\n",
    "#     num_train_epochs=script_args.num_train_epochs,\n",
    "#     weight_decay=script_args.weight_decay,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     eval_steps=script_args.eval_every_steps,\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=script_args.save_every_steps,\n",
    "#     gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    "#     gradient_checkpointing=script_args.gradient_checkpointing,\n",
    "#     deepspeed=script_args.deepspeed,\n",
    "#     local_rank=script_args.local_rank,\n",
    "#     remove_unused_columns=False,\n",
    "#     label_names=[],\n",
    "#     bf16=script_args.bf16,\n",
    "#     logging_strategy=\"steps\",\n",
    "#     logging_steps=10,\n",
    "#     optim=script_args.optim,\n",
    "#     lr_scheduler_type=script_args.lr_scheduler_type,\n",
    "#     warmup_ratio=0.03,\n",
    "#     report_to='wandb'\n",
    "# )\n",
    "\n",
    "# enable if you want to train with lora\n",
    "# peft_config = LoraConfig(\n",
    "#     task_type=TaskType.SEQ_CLS,\n",
    "#     inference_mode=False,\n",
    "#     r=8,\n",
    "#     lora_alpha=32,\n",
    "#     lora_dropout=0.1,\n",
    "# )\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    script_args.model_name, num_labels=1, torch_dtype=torch.bfloat16, cache_dir = FILE_PATH\n",
    ")\n",
    "# model = get_peft_model(model, peft_config)\n",
    "# model.print_trainable_parameters()\n",
    "print(\"model loaded \", model)\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.exit(0)\n",
    "\n",
    "model.config.use_cache = not script_args.gradient_checkpointing\n",
    "num_proc = 24  # Can adjust to be higher if you have more processors.\n",
    "original_columns = train_dataset.column_names\n",
    "\n",
    "\n",
    "# We need to define a special data collator that batches the data in our j vs k format.\n",
    "@dataclass\n",
    "class RewardDataCollatorWithPadding:\n",
    "    tokenizer: AutoTokenizer\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        merged_features = []\n",
    "        for feature in features:\n",
    "            merged_features.append(\n",
    "                {\n",
    "                    \"input_ids\": feature[\"input_ids_j\"],\n",
    "                    \"attention_mask\": feature[\"attention_mask_j\"],\n",
    "                }\n",
    "            )\n",
    "            merged_features.append(\n",
    "                {\n",
    "                    \"input_ids\": feature[\"input_ids_k\"],\n",
    "                    \"attention_mask\": feature[\"attention_mask_k\"],\n",
    "                }\n",
    "            )\n",
    "        batch = self.tokenizer.pad(\n",
    "            merged_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        batch = {\n",
    "            \"input_ids\": batch[\"input_ids\"],\n",
    "            \"attention_mask\": batch[\"attention_mask\"],\n",
    "            \"return_loss\": True,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "\n",
    "# Define the trainer\n",
    "def compute_metrics(eval_pred):\n",
    "    result = {}\n",
    "    pos_predictions_scores = eval_pred.predictions[0]\n",
    "    neg_predictions_scores = eval_pred.predictions[1]\n",
    "    # We assume that the first sample is preferred by default in groundtruth\n",
    "    result['accuracy'] = np.sum(\n",
    "        pos_predictions_scores > neg_predictions_scores) / len(pos_predictions_scores)\n",
    "    return result\n",
    "\n",
    "\n",
    "class RewardTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        rewards = model(\n",
    "            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n",
    "        )[0]\n",
    "        bsz = rewards.size(0)\n",
    "        jidx = torch.arange(0, bsz, 2)\n",
    "        kidx = jidx + 1\n",
    "        rewards_j = rewards[jidx]\n",
    "        rewards_k = rewards[kidx]\n",
    "        loss = -nn.functional.logsigmoid(rewards_j - rewards_k).mean()\n",
    "        if return_outputs:\n",
    "            return loss, {\"rewards_j\": rewards_j, \"rewards_k\": rewards_k}\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Train the model, woohoo.\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=RewardDataCollatorWithPadding(\n",
    "        tokenizer=tokenizer, max_length=script_args.max_length),\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "print(\"Saving last checkpoint of the model\")\n",
    "#model.save_pretrained(output_name + \"/last_checkpoint\")\n",
    "trainer.save_model(output_name + \"/last_checkpoint\")\n",
    "tokenizer.save_pretrained(output_name + \"/last_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0317e76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d9a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8160511f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95f010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19934df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
