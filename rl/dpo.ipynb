{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d6fa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = 'google/gemma-3-270m-it'\n",
    "DATASET_NAME = 'Dahoas/rm-static'\n",
    "from transformers import AutoTokenizer , AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import DPOConfig , DPOTrainer\n",
    " \n",
    "import os \n",
    "\n",
    "FILE_PATH = os.path.dirname(os.path.abspath(\"\"))\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level = logging.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f6430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing dataset \n",
    "train_dataset = load_dataset(DATASET_NAME , cache_dir = FILE_PATH, split='train[:5%]')\n",
    "\n",
    "\n",
    "print(type(train_dataset))\n",
    "\n",
    "# train_dataset = train_dataset[0:500] #only take first 500 examples \n",
    "# print(type(train_dataset)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aa3200",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa67dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and tokenizer \n",
    "\n",
    "\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, cache_dir = FILE_PATH, attn_implementation='eager')\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL , cache_dir = FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b641f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vizualize_tokenizer \n",
    "import json\n",
    "\n",
    "cfg_path = os.path.join(FILE_PATH+'/Dahoas__rm-static/', \"tokenizer_config.json\")\n",
    "print('cfg path', cfg_path)\n",
    "if os.path.isfile(cfg_path):\n",
    "    print(\"tokenizer_config.json:\")\n",
    "    print(json.dumps(json.load(open(cfg_path)), indent=2))\n",
    "\n",
    "prompt = 'Hi Chat, How are you doing ?'\n",
    "prompt2 = 'Hello world ! I am here'\n",
    "out2 = tokenizer(prompt2)\n",
    "out = tokenizer(prompt)\n",
    "\n",
    "print(out)\n",
    "print(out2)\n",
    "\n",
    "print(tokenizer.bos_token_id, tokenizer.pad_token_id)\n",
    "print(policy_model.config.bos_token_id, policy_model.config.pad_token_id)\n",
    "\n",
    "\n",
    "if hasattr(policy_model, \"generation_config\") and policy_model.generation_config is not None:\n",
    "    policy_model.generation_config.bos_token_id = tokenizer.bos_token_id\n",
    "    policy_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "print('\\n\\n---- TOKENIZER AND MODELS ----')\n",
    "print(\"tokenizer vocab_size:\", len(tokenizer))\n",
    "print(\"model.config.vocab_size:\", getattr(policy_model.config, \"vocab_size\", None))\n",
    "print(\"embeddings rows:\", policy_model.get_input_embeddings().weight.shape[0])\n",
    "\n",
    "print(\"tokenizer special map:\", tokenizer.special_tokens_map)\n",
    "print(\"tokenizer ids: bos, pad, eos =\", tokenizer.bos_token_id, tokenizer.pad_token_id, tokenizer.eos_token_id)\n",
    "print(\"model ids:     bos, pad, eos =\", policy_model.config.bos_token_id, policy_model.config.pad_token_id, policy_model.config.eos_token_id)\n",
    "print(\"generation_config:\", getattr(policy_model, \"generation_config\", None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13868f0",
   "metadata": {},
   "source": [
    "model config just stores what were the bos , eos tokens used while training this model (not the actual token strings) so if you are planning to use another tokenizer set to finetune , match these up and if possible try to use the same tokenizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59705b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model.generation_config.eos_token_id = ['1']\n",
    "print(\"generation_config:\", getattr(policy_model, \"generation_config\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dfbbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the config file for the tokenizer \n",
    "# find the extra id & token\n",
    "extra_id = len(tokenizer) - 1\n",
    "extra_token = tokenizer.convert_ids_to_tokens(extra_id)\n",
    "print(\"extra_id:\", extra_id, \"extra_token:\", extra_token)\n",
    "\n",
    "# check if it is an added token or a special token\n",
    "print(\"is special token?\", extra_token in tokenizer.all_special_tokens)\n",
    "print(\"added tokens:\", tokenizer.get_added_vocab())   # dict token->id for user-added tokens\n",
    "print(\"appears in vocab?\", extra_token in tokenizer.get_vocab())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e678cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model.config.tokenizer_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f16bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = DPOConfig(\n",
    "    do_train = True,\n",
    "    per_device_train_batch_size=8,\n",
    "    learning_rate=5e-8,\n",
    "    bf16 = False,\n",
    "    fp16=False,\n",
    "    logging_strategy='steps', \n",
    "    logging_steps=2, \n",
    ")\n",
    "\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model = policy_model, \n",
    "    args = config, \n",
    "    train_dataset = train_dataset\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3b1e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "del policy_model , tokenizer\n",
    "\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce27b014",
   "metadata": {},
   "source": [
    "## DPO from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2867e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model , tokenizer\n",
    "\n",
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc10791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets now self create the model and see what actually works \n",
    "'''\n",
    "Preference / Reward model \n",
    "Here it learns to reward the \n",
    "\n",
    "** Dataset ** \n",
    "\n",
    "Prompt : Why is the color of sky blue ?\n",
    "Chosen : Its because of the scattering of blue wavelength by air molecules \n",
    "Rejected : Because sky likes blue color\n",
    "\n",
    "\n",
    "The reward model, is a linear layer at top, that learn to output whether this response is good or not \n",
    "\n",
    "Input1 to model : {Prompt + Chosen} ,  Output1 : 1  \n",
    "Input2 to model : {Prompt + Rejected}  , Output2: 0 \n",
    "\n",
    "\n",
    "Here the model learns to pick up a good response !\n",
    "\n",
    "This is the logic that happens in PPO loop ! So there is nothing as seperate training for a PPO model directly just use this as a Value head  \n",
    "'''\n",
    "\n",
    "# Single runnable cell â€” minimal, targeted fixes only (copy-paste)\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoConfig,\n",
    "    PreTrainedModel,\n",
    "    GenerationConfig\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from transformers.modeling_outputs import ModelOutput\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# ------------------ USER-CHOICE (preserved) ------------------\n",
    "BASE_MODEL = \"google/gemma-3-270m-it\"\n",
    "DATASET_LINK = \"Dahoas/rm-static\"\n",
    "\n",
    "import os \n",
    "FILE_PATH = os.path.dirname(os.path.abspath(\"\"))\n",
    "print(FILE_PATH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd76d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL , cache_dir = FILE_PATH)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, cache_dir = FILE_PATH)\n",
    "\n",
    "dataset = load_dataset(DATASET_LINK, split=\"train[:20]\", cache_dir=FILE_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4edb0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenizer_mapping(example): \n",
    "  print(\"example is \", example)\n",
    "  prompt = example['prompt']\n",
    "  response = example['response']\n",
    "  rejected = example['rejected']\n",
    "  \n",
    "  prompt_ids = tokenizer(prompt, add_special_tokens=False).input_ids\n",
    "  response_ids   = tokenizer(response + tokenizer.eos_token, add_special_tokens=False).input_ids\n",
    "  rejected_ids = tokenizer(rejected + tokenizer.eos_token, add_special_tokens= False).input_ids\n",
    "\n",
    "\n",
    "  question_correct_ids  = prompt_ids + response_ids\n",
    "  question_rejected_ids = prompt_ids + rejected_ids\n",
    "\n",
    "\n",
    "  attention_mask_correct_ids  = [1 ]* len(question_correct_ids)\n",
    "  attention_mask_rejected_ids = [1] * len(question_rejected_ids)\n",
    "\n",
    "\n",
    "  resp_mask_corrected = [0] * len(prompt_ids) + [1] * len(response_ids)\n",
    "  resp_mask_rejected = [0] * len(prompt_ids) + [1] * len(rejected_ids)\n",
    "\n",
    "\n",
    "  assert len(question_correct_ids) == len(resp_mask_corrected) ,\"These should be equal but are unequal !\"  \n",
    "  assert len(question_rejected_ids) == len(resp_mask_rejected) ,\"These should be equal but are unequal !\"  \n",
    "\n",
    "  return { \n",
    "    'correct_input_ids'         : torch.tensor(data = question_correct_ids , dtype= torch.int32), \n",
    "    'rejected_input_ids'        : torch.tensor(data = question_rejected_ids , dtype= torch.int32), \n",
    "    'attention_mask_corrected'       : torch.tensor(data = attention_mask_correct_ids , dtype=torch.int32), \n",
    "    'attention_mask_rejected'        : torch.tensor(data = attention_mask_rejected_ids, dtype=torch.int32), \n",
    "    'resp_mask_corrected'       : torch.tensor(data = resp_mask_corrected , dtype = torch.int32),\n",
    "    'resp_mask_rejected'        : torch.tensor(data = resp_mask_rejected , dtype = torch.int32),\n",
    "  }\n",
    "\n",
    "revised_dataset = dataset.map(tokenizer_mapping)\n",
    "\n",
    "print(revised_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d150e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf5b9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(revised_dataset[0].get('input_ids')) , len(revised_dataset[0].get('attention_mask')) , len(revised_dataset[1].get('input_ids')) , len(revised_dataset[1].get('attention_mask')) \n",
    "len(revised_dataset['input_ids']) , len(revised_dataset['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5089678",
   "metadata": {},
   "source": [
    "DPO : here we have the choosen response and the rejected one , that is , what is the prob of getting a choosen response and what is the prob of getting a rejected response\n",
    "\n",
    "x: input / prompt \n",
    "yc : correct output\n",
    "yr : rejected output\n",
    "\n",
    "> Choosen : Prob (Y_c | X ) , Prob (Y_r | X )\n",
    "\n",
    "Prob (Y_c | X ) : sum of prob of the individual output tokens ..   \n",
    "\n",
    "> rejected  : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784a4a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "logp = None\n",
    "\n",
    "def next_token_prediction_values(logp:torch.Tensor, idx :int ) -> torch.Tensor:\n",
    "    return logp[:, idx, :] # output will be torch tensor \n",
    "\n",
    "# resp_mask this is the response mask that means it tells which parts of the prompt to take in the input and which parts not to take !!\n",
    "def seq_logprob(model , input_ids , attention_mask, resp_mask):\n",
    "    '''\n",
    "    This is the log prob, this is the \n",
    "    '''\n",
    "    \n",
    "    if type(input_ids) is not torch.Tensor:\n",
    "        print(input_ids)\n",
    "        print(type(input_ids))\n",
    "        input_ids = torch.tensor(data = input_ids, dtype = torch.int32)\n",
    "        attention_mask = torch.tensor(data = attention_mask, dtype=torch.int32)\n",
    "\n",
    "    out = model(input_ids = input_ids.unsqueeze(0), attention_mask = attention_mask.unsqueeze(0))\n",
    "    print(\"out logits shape is \", out.logits.shape) # outputs the probability for all the input tokens , the next one will only be from the last one\n",
    "\n",
    "    # In the DPO, I am already passing the input to the model / the output also in the prompt itself ..  \n",
    "    logp = F.log_softmax(out.logits, dim = -1)\n",
    "    print(\"full logp shape is : \", logp.shape) # log outputs for all the next tokens  \n",
    "\n",
    "    # now output the dpo probs\n",
    "        \n",
    "\n",
    "    # Now I need to find the last one, so we are taking the probability of the whole sequence .. the input + output ( prompt + response) \n",
    "    # global logp\n",
    "    logp = F.log_softmax(out.logits, dim = -1)\n",
    "    print('logp shape is : ' , logp.shape) # 1 x 163 x VS\n",
    "\n",
    "    # logp: here tells the probabity values of the next tokens coming out from these \n",
    "    # !! next_token_prediction_values(logp, idx)\n",
    "\n",
    "    output_model_prob = []\n",
    "    for i,val in enumerate(resp_mask):\n",
    "        if val == 0: \n",
    "            continue\n",
    "        else:\n",
    "            # print(\"the value is : \", val)\n",
    "            prob_distribution = next_token_prediction_values(logp, i-1).squeeze(0) # so to get prev one -> 1xVS \n",
    "            # print(\"prob distribution is\", prob_distribution.shape)\n",
    "            correct_token_idx = input_ids[i] # correct / incorrect input idx ..  \n",
    "            val = prob_distribution[correct_token_idx]\n",
    "            print('Token should have been : ', tokenizer.decode([correct_token_idx]), ' The probability I am getting out from this is : ', val.item()) \n",
    "            output_model_prob.append(val)\n",
    "\n",
    "    print(type(output_model_prob))\n",
    "    \n",
    "    return torch.stack(output_model_prob).mean()\n",
    "\n",
    "    # --- # --- #\n",
    "\n",
    "\n",
    "# seq_logprob(model, input_ids = revised_dataset[0]['input_ids'], attention_mask = revised_dataset[0]['attention_mask'], resp_mask= revised_dataset[0]['resp_mask'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368bbf70",
   "metadata": {},
   "source": [
    "\"<bos><start_of_turn>user\\nHello, how are you?<end_of_turn>\\n<start_of_turn>model\\nI'm doing great. How can I help you today?<end_of_turn>\\n<start_of_turn>user\\nI'd like to show off how chat templating works!<end_of_turn>\\n<start_of_turn>model\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3611d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# policy_model = AutoModelForCausalLM.from_pretrained()\n",
    "# reference_model = AutoModelForCausalLM.from_pretrained()\n",
    "#     features: ['prompt', 'response', 'chosen', 'rejected', 'correct_input_ids', 'rejected_input_ids', 'attention_mask_corrected', 'attention_mask_rejected', 'resp_mask_corrected', 'resp_mask_rejected'],\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import copy\n",
    "policy_model = model\n",
    "reference_model = copy.deepcopy(model).eval()\n",
    "\n",
    "for param in reference_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    param.grad = None\n",
    "\n",
    "optim = torch.optim.AdamW(policy_model.parameters(), lr = 5e-8)\n",
    "\n",
    "def dpo_loss(policy_correct_log, policy_rejected_log ,reference_correct_log, reference_rejected_log, beta = 0.1):\n",
    "    # these are all log \n",
    "    v1 = beta * (policy_rejected_log - reference_rejected_log - policy_correct_log + reference_correct_log)\n",
    "    return -F.logsigmoid(v1).mean()\n",
    "\n",
    "\n",
    "# dataset = dataset.map(tokenizer_mapping)\n",
    "\n",
    "device = 'cpu'\n",
    "policy_model = policy_model.to(device)\n",
    "reference_model = reference_model.to(device) \n",
    "\n",
    "def training_loop(epochs = 1): \n",
    "    print('Starting the training loop')\n",
    "    for idx, data in enumerate(dataset):\n",
    "        print(\"data is : \", data)\n",
    "        optim.zero_grad() # we dont like accumulation !! \n",
    "        correct_input_ids = data['correct_input_ids']\n",
    "        rejected_input_ids = data['rejected_input_ids']\n",
    "        \n",
    "        attention_mask_corrected = data['attention_mask_corrected']\n",
    "        attention_mask_rejected = data['attention_mask_rejected']\n",
    "\n",
    "        resp_mask_corrected = data['resp_mask_corrected']\n",
    "        resp_mask_rejected = data['resp_mask_rejected']\n",
    "\n",
    "        # Forward pass (policy)\n",
    "        policy_correct_log = seq_logprob(policy_model, correct_input_ids, attention_mask_corrected, resp_mask_corrected)\n",
    "        policy_rejected_log = seq_logprob(policy_model, rejected_input_ids, attention_mask_rejected, resp_mask_rejected)\n",
    "\n",
    "        # Forward pass (reference, no grad)\n",
    "        with torch.inference_mode():\n",
    "            reference_correct_log = seq_logprob(reference_model, correct_input_ids, attention_mask_corrected, resp_mask_corrected)\n",
    "            reference_rejected_log = seq_logprob(reference_model, rejected_input_ids, attention_mask_rejected, resp_mask_rejected)\n",
    "\n",
    "        # Loss + backward\n",
    "        loss = dpo_loss(policy_correct_log, policy_rejected_log, reference_correct_log, reference_rejected_log)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if idx % 50 == 0:\n",
    "            print(f\"Epoch {epochs} | Step {idx} | Loss {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "training_loop(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b481f851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
