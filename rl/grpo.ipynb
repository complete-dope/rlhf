{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets now self create the model and see what actually works \n",
    "'''\n",
    "Preference / Reward model \n",
    "Here it learns to reward the \n",
    "\n",
    "** Dataset ** \n",
    "\n",
    "Prompt : Why is the color of sky blue ?\n",
    "Chosen : Its because of the scattering of blue wavelength by air molecules \n",
    "Rejected : Because sky likes blue color\n",
    "\n",
    "\n",
    "The reward model, is a linear layer at top, that learn to output whether this response is good or not \n",
    "\n",
    "Input1 to model : {Prompt + Chosen} ,  Output1 : 1  \n",
    "Input2 to model : {Prompt + Rejected}  , Output2: 0 \n",
    "\n",
    "\n",
    "Here the model learns to pick up a good response !\n",
    "\n",
    "This is the logic that happens in PPO loop ! So there is nothing as seperate training for a PPO model directly just use this as a Value head  \n",
    "'''\n",
    "\n",
    "# Single runnable cell â€” minimal, targeted fixes only (copy-paste)\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoConfig,\n",
    "    PreTrainedModel,\n",
    "    GenerationConfig\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from transformers.modeling_outputs import ModelOutput\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# ------------------ USER-CHOICE (preserved) ------------------\n",
    "BASE_MODEL = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "DATASET_LINK = \"Dahoas/rm-static\"\n",
    "DEVICE = 'mps'\n",
    "\n",
    "\n",
    "import os \n",
    "FILE_PATH = os.path.dirname(os.path.abspath(\"\"))\n",
    "print(FILE_PATH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb24bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL , cache_dir = FILE_PATH)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, cache_dir = FILE_PATH)\n",
    "\n",
    "dataset = load_dataset(DATASET_LINK, split=\"train[:20]\", cache_dir=FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ceded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model , tokenizer , dataset \n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c7426",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5bf3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(example):\n",
    "    prompt = tokenizer.bos_token + example['prompt']\n",
    "    example['prompt'] = prompt\n",
    "    tokenized_example =  tokenizer(prompt)\n",
    "    input_ids = tokenized_example.input_ids\n",
    "    attention_mask = tokenized_example.attention_mask\n",
    "    mask = torch.tensor([1] * len(input_ids))\n",
    "\n",
    "    return {\n",
    "        'input_ids':input_ids, \n",
    "        'resp_mask': mask, \n",
    "        'attention_mask':mask\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(prepare_dataset)\n",
    "\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ce6570",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "# logp = None\n",
    "\n",
    "# def next_token_prediction_values(logp:torch.Tensor, idx :int ) -> torch.Tensor:\n",
    "#     return logp[:, idx, :] # output will be torch tensor \n",
    "\n",
    "# # resp_mask this is the response mask that means it tells which parts of the prompt to take in the input and which parts not to take !!\n",
    "# def seq_logprob(model , input_ids , attention_mask, resp_mask):\n",
    "#     '''\n",
    "#     This is the log prob, this is the \n",
    "#     '''\n",
    "    \n",
    "#     if type(input_ids) is not torch.Tensor:\n",
    "#         print(input_ids)\n",
    "#         print(type(input_ids))\n",
    "#         input_ids = torch.tensor(data = input_ids, dtype = torch.int32)\n",
    "#         attention_mask = torch.tensor(data = attention_mask, dtype=torch.int32)\n",
    "\n",
    "#     out = model(input_ids = input_ids.unsqueeze(0), attention_mask = attention_mask.unsqueeze(0))\n",
    "#     print(\"out logits shape is \", out.logits.shape) # outputs the probability for all the input tokens , the next one will only be from the last one\n",
    "\n",
    "#     # In the DPO, I am already passing the input to the model / the output also in the prompt itself ..  \n",
    "#     logp = F.log_softmax(out.logits, dim = -1)\n",
    "#     print(\"full logp shape is : \", logp.shape) # log outputs for all the next tokens  \n",
    "\n",
    "#     # Now I need to find the last one, so we are taking the probability of the whole sequence .. the input + output ( prompt + response) \n",
    "#     # global logp\n",
    "#     logp = F.log_softmax(out.logits, dim = -1)\n",
    "#     print('logp shape is : ' , logp.shape) # 1 x 163 x VS\n",
    "\n",
    "#     output_model_prob = []\n",
    "#     for i,val in enumerate(resp_mask):\n",
    "#         if val == 0: \n",
    "#             continue\n",
    "#         else:\n",
    "#             prob_distribution = next_token_prediction_values(logp, i-1).squeeze(0) # so to get prev one -> 1xVS \n",
    "#             correct_token_idx = input_ids[i] # correct / incorrect input idx ..  \n",
    "#             val = prob_distribution[correct_token_idx]\n",
    "#             print('Token should have been : ', tokenizer.decode([correct_token_idx]), ' The probability I am getting out from this is : ', val.item()) \n",
    "#             output_model_prob.append(val)\n",
    "\n",
    "#     print(type(output_model_prob))\n",
    "    \n",
    "#     return torch.stack(output_model_prob).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b42ba3",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4313c76b",
   "metadata": {},
   "source": [
    "\n",
    "Prompt: 'How to pet my dog?'\n",
    "\n",
    "Answer: \n",
    "\n",
    "token-1 : 'Petting'\n",
    "token-2 : 'your'\n",
    "token-3 : 'dog'\n",
    "token-4 : 'is'\n",
    "token-5 : 'a'\n",
    "token-6 : 'good'\n",
    "token-7 : 'relaxation'\n",
    "... etc \n",
    "\n",
    "\n",
    "\n",
    "output 1 : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58d1736",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "reward_model = AutoModelForCausalLMWithValueHead.from_pretrained(BASE_MODEL, cache_dir = FILE_PATH)\n",
    "\n",
    "reward_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c8666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vals = 'I am a good boy'\n",
    "ds = tokenizer(vals).input_ids\n",
    "ds = torch.tensor(data = ds).unsqueeze(0).to(DEVICE)\n",
    "a = reward_model.forward(ds)\n",
    "# print(a[1][-1])\n",
    "a[2][:,-1]\n",
    "\n",
    "# tokenizer.decode(*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52f62e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[2], a[2][:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56a3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import GenerationConfig\n",
    "# https://huggingface.co/docs/transformers/en/main_classes/text_generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cbcd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sampling_outputs(model , data , n=5):\n",
    "    generation_config = GenerationConfig.from_pretrained(BASE_MODEL, cache_dir=FILE_PATH)\n",
    "    \n",
    "    generation_config.max_new_length = 400\n",
    "    generation_config.temperature= 0.6\n",
    "    generation_config.use_cache = False\n",
    "    generation_config.do_sample = True\n",
    "    \n",
    "    input_ids = torch.tensor(data['input_ids']).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    outs = []\n",
    "    for _ in range(n):\n",
    "        # print('inside the loop !!')\n",
    "        # print(\"The input to the model is \" , tokenizer.decode(*input_ids) )\n",
    "        out = model.generate(input_ids , generation_config = generation_config)\n",
    "        generated_output = out[:, input_ids.shape[1]:]\n",
    "\n",
    "        # print('model outputs is : ',  generated_output  , generated_output.device)\n",
    "        # out = tokenizer.decode(*out) # this is giving the whole input + the output why ??\n",
    "        out = tokenizer.decode(*generated_output) # this is giving the whole input + the output why ??\n",
    "        \n",
    "        outs.append(generated_output)\n",
    "\n",
    "    return outs\n",
    "\n",
    "\n",
    "def scoring_outputs(reward_model, outputs):\n",
    "    \n",
    "    # if type(outputs) == list:\n",
    "    #     print(\"Type of outputs is : \", type(outputs), outputs)\n",
    "\n",
    "    rewards = []\n",
    "    for output in outputs: \n",
    "        tokenized_data = output\n",
    "        input_ids = tokenized_data\n",
    "        attention_mask = [1] * output.shape[-1]\n",
    "        attention_mask = torch.tensor(data = attention_mask).to(DEVICE).unsqueeze(0)\n",
    "\n",
    "        reward = reward_model.forward(input_ids , attention_mask = attention_mask)\n",
    "        reward = reward[2][:,-1]\n",
    "        rewards.append(reward)\n",
    "\n",
    "    return rewards        \n",
    "    \n",
    "\n",
    "def group_computation(advantages) -> torch.Tensor:\n",
    "    '''\n",
    "    Group computation \n",
    "    '''\n",
    "    advantages = torch.stack(advantages).float()\n",
    "    return  (advantages - advantages.mean()) / advantages.std()\n",
    "\n",
    "\n",
    "data = torch.tensor(data= 1)\n",
    "data2 = torch.tensor(data= 2)\n",
    "reward = [1,2,3,4,45]\n",
    "reward = [data, data, data, data2]\n",
    "\n",
    "print(group_computation(reward))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f889587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_loss(prompt: torch.Tensor, outputs:torch.Tensor, advantages:torch.Tensor, policy_model , reference_model, old_model, beta:torch.Tensor):\n",
    "    '''\n",
    "    GRPO loss \n",
    "    so its requires the prompt , outputs and averages over the output values \n",
    "\n",
    "    loss goes like this :\n",
    "    old_policy : keep the t-1 policy weights as we need to importance sampling ( # TODO : more on this )\n",
    "    policy : the one for which we are optimizing \n",
    "    reference model : the reference / fixed model \n",
    "    '''\n",
    "    import gc \n",
    "    gc.collect()\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "\n",
    "    # print('type of prompt is : ', type(prompt), prompt)\n",
    "    # print('type of outputs is : ', type(outputs), outputs)\n",
    "    # print('type of advantages is : ', type(advantages), advantages)\n",
    "    # print('type of beta is : ', type(beta))\n",
    "    \n",
    "    if type(prompt) is list: \n",
    "        prompt = torch.tensor(prompt).to(DEVICE)\n",
    "\n",
    "    # if type(beta) is float:\n",
    "    beta = torch.tensor(beta).to(DEVICE)\n",
    "    \n",
    "    # print('type of prompt is : ', type(prompt), prompt)\n",
    "    # print('beta shape is ', beta.shape)\n",
    "\n",
    "    policy_model.train()\n",
    "    reference_model.eval()\n",
    "    old_model.eval()\n",
    "\n",
    "    \n",
    "    assert len(outputs) == len(advantages) , 'The no. of samples of the outputs generated should be equal to no. of advantages processed '\n",
    "\n",
    "    loss = torch.tensor(data = 0.0).to(DEVICE)\n",
    "    for idx, output in enumerate(outputs): \n",
    "        advantage = advantages[idx]\n",
    "        output = output.squeeze(0).to(DEVICE)\n",
    "        total_sum_of_probs = torch.tensor(data = 0.0).to(DEVICE)\n",
    "        for t, output_token in enumerate(output): \n",
    "            # check the probability of getting this token , given input prompt + tokens till this\n",
    "            # print('prompt is ', type(prompt) , prompt)\n",
    "            # print('output tokens are : ', type(output[:t]), output[:t])\n",
    "            input_tokens = torch.cat((prompt, output[:t])).unsqueeze(0)\n",
    "            policy_model_prob = policy_model(input_tokens)\n",
    "\n",
    "\n",
    "            policy_model_prob = policy_model_prob.logits[:,-1, output_token] # Output will be a tensor of Vocab_Size (VS)\n",
    "            with torch.no_grad():\n",
    "                reference_model_prob = reference_model(input_tokens).logits[:,-1, output_token]\n",
    "                old_model_prob = old_model(input_tokens).logits[:,-1, output_token]\n",
    "\n",
    "            assert policy_model_prob.shape == reference_model_prob.shape , 'The output shapes of the probabilities should be of same shape '\n",
    "\n",
    "            first_term = ( policy_model_prob / old_model_prob ) * advantage  \n",
    "            second_term = beta * (reference_model_prob / policy_model_prob)\n",
    "            third_term = beta * (torch.log(reference_model_prob / policy_model_prob))\n",
    "            fourth_term = beta\n",
    "\n",
    "            ans = (first_term - second_term + third_term + fourth_term).squeeze()\n",
    "            total_sum_of_probs += ans\n",
    "\n",
    "            # Clean up explicitly if needed\n",
    "            del input_tokens, policy_model_prob, reference_model_prob, old_model_prob\n",
    "            torch.mps.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "        loss += total_sum_of_probs / len(output)\n",
    "    \n",
    "    return loss / len(outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb084f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[16096,    28,   338,   417,    99,   253,  1109,  2359,    30, 17579,\n",
    "           253,  1165,  1902,   282,   314,   371, 11899,    92,  4657,   288]]\n",
    "\n",
    "a = torch.tensor(data)\n",
    "a[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc89616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfad47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_ids = torch.tensor(dataset[0]['input_ids']).unsqueeze(0)\n",
    "attention_mask = torch.tensor(dataset[0]['attention_mask']).unsqueeze(0)\n",
    "print(input_ids)\n",
    "outs =model.generate(input_ids)\n",
    "print(outs.shape)\n",
    "\n",
    "print(outs)\n",
    "print(tokenizer.decode(*outs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05be514",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a498ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gc \n",
    "gc.collect()\n",
    "torch.mps.empty_cache()\n",
    "\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "#model definition \n",
    "policy_model = model\n",
    "reference_model = copy.deepcopy(model).eval()\n",
    "reward_model = reward_model\n",
    "\n",
    "for param in reference_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    param.grad = None\n",
    "\n",
    "optim = torch.optim.AdamW(policy_model.parameters(), lr = 5e-8)\n",
    "\n",
    "# dataset = dataset.map(tokenizer_mapping)\n",
    "device = DEVICE\n",
    "policy_model = policy_model.to(device)\n",
    "reference_model = reference_model.to(device) \n",
    "reward_model = reward_model.to(device)\n",
    "# dataset = dataset.to(device)\n",
    "\n",
    "print(\"The models are on : \", policy_model.device)\n",
    "\n",
    "outputs = None\n",
    "\n",
    "def training_loop(epochs = 1): \n",
    "    global outputs\n",
    "    for epoch in range(0, epochs):\n",
    "        for idx, data in enumerate(dataset):\n",
    "            gc.collect()\n",
    "            torch.mps.empty_cache()\n",
    "\n",
    "            optim.zero_grad() # we dont like accumulation !! \n",
    "    \n",
    "            outputs = sampling_outputs(policy_model, data, n=4)                    \n",
    "            rewards = scoring_outputs(reward_model, outputs)\n",
    "\n",
    "            assert len(outputs) == len(rewards) , \"The shape mismatch should not occur\"\n",
    "            advantages = group_computation(rewards)\n",
    "            \n",
    "            # Get the final advantages here !! \n",
    "\n",
    "            # loss function implementation !! from : https://arxiv.org/pdf/2402.03300\n",
    "\n",
    "            loss = grpo_loss(data['input_ids'] , outputs , advantages, policy_model , policy_model , policy_model ,  beta=0.4)\n",
    "            print('LOSS : ' ,loss.item() , 'EPOCH : ', epoch)\n",
    "\n",
    "            loss.backward() # cals done\n",
    "            optim.step() # backprop\n",
    "\n",
    "            break\n",
    "\n",
    "training_loop(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1069cfa0",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/2402.03300"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
